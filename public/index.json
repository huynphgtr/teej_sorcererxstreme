[
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/",
	"title": "AI Development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/",
	"title": "Backend Development",
	"tags": [],
	"description": "",
	"content": "Building and Automating with Serverless Backend (Serverless V4) This guide is a comprehensive document detailing the entire Backend development process for the SorcererXtreme AI project, from setting up the local development environment to completing the automated deployment workflow (CI/CD) on AWS.\n1. Context \u0026amp; Technical Challenges In the development of high-performance Serverless applications, we face several complex technical hurdles:\nFramework Transition: The transition from a traditional HTTP Framework like Express.js to the stateless environment of AWS Lambda requires using serverless-http and a change in source code structure. Database Management: Using a sophisticated ORM like Prisma on a Serverless platform necessitates techniques to optimize the deployment package size to under 250MB and requires downloading the correct Prisma Binary (rhel-openssl-3.0.x) for the Lambda Linux environment. Security: Ensuring that sensitive connection strings are never stored in the source code, but are securely managed using AWS SSM Parameter Store. 2. Core Value of the Constructed Architecture Our Backend architecture is designed to overcome these challenges, delivering key benefits:\nCost and Performance Optimization: Utilizing AWS Lambda and Serverless Framework V4 ensures you only pay for the time your code actually runs. Optimizing the Prisma package significantly reduces Cold Start time. Full Automation (CI/CD): Building the GitHub Actions workflow automates the entire development loop: Code -\u0026gt; Push -\u0026gt; Build -\u0026gt; Deploy, completely eliminating manual deployment steps and minimizing human error. Data Flexibility: Establishing a stable and secure connection to an external Database (NeonDB) demonstrates the capability for flexible integration in real-world projects. This guide will serve as a detailed, step-by-step map to help you master the entire process of modern Serverless Backend development.\nPrepare Set up environment Configure Serverless Framework Save secret key Automation CI/CD Connect microservices Set up email Development workflow "
},
{
	"uri": "http://localhost:1313/repo-name/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Group Internship Details Project Information Field Detail Group name TEEJ_SorcererXStreme University FPT University - Ho Chi Minh Campus Internship company Amazon Web Services Vietnam Co., Ltd. Internship position FCJ Workforce Program Intern Internship duration From 08/09/2025 to 24/12/2025 Team Member Photo Role Full Name Major Contact Leader Tran Phuong Huyen Software Engineering tranphuonghuyen2005@gmail.com AI Nguyen Lam Anh Artifical Intelligent nguyenla110505@gmail.com AI Nguyen Van Linh Artifical Intelligent nguyenvanlinh.1710.it@gmail.com SE Bui Nguyen Tan Khang Software Engineering tankhang6a6@gmail.com "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme AI: Building an AI-Powered Metaphysical Guidance Platform on AWS The core purpose of this project, for a developer workshop, is to demonstrate how to build a scalable, cost-optimized, multi-faceted application capable of handling complex data flows entirely within the cloud environment.\nProblem Solved \u0026amp; Technical Value The Challenge: Building a platform that combines the need for precise computation with the linguistic creativity of AI, while ensuring all content is verifiable and grounded. Traditional server-based solutions often struggle with the dynamic scaling required for such varied workloads. The Technical Solution: We solve this by implementing a Retrieval-Augmented Generation (RAG) Core utilizing Amazon Bedrock and Pinecone. This design allows the AI to produce verified answers based on a specialized knowledge base, transforming speculative guidance into actionable insights. Key Technical Highlights This project serves as an essential case study for integrating the following critical AWS services:\nServerless Compute: We utilize AWS Lambda as the entire Backend, eliminating server management overhead and significantly optimizing costs. Modern Deployment (Frontend Hosting): Deploying the Next.js application on AWS Amplify provides streamlined CI/CD and hosting for the Frontend. Durable Asynchronous Flow (Async): We constructed a reliable automated reminder system using EventBridge -\u0026gt; Lambda -\u0026gt; SES. This pattern ensures robust, scalable bulk delivery without overloading the core API. Data Persistence and Vectors: We manage complex relational data externally using NeonDB (Serverless PostgreSQL) while utilizing a specialized Vector Database (Pinecone) for the high-speed RAG retrieval layer. Security and DevOps: AWS Parameter Store manages all sensitive keys, and the entire infrastructure is deployed using the Serverless Framework driven by GitHub Actions (CI/CD). Best Practices Learned Attendees will learn how to implement a fully Serverless Microservices architecture, address challenges like external database connectivity, splitting synchronous/asynchronous workloads, and building a cost-effective RAG Core solution.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.1-prepare/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "1. Technologies Used Category Technology Detail \u0026amp; Role Language TypeScript (Node.js 20) Primary language, providing Type Safety. Backend Core Express.js + serverless-http Familiar API Framework, \u0026ldquo;wrapped\u0026rdquo; to run on Lambda. Infrastructure (IaC) Serverless Framework V4 Main tool for defining and deploying the entire AWS architecture. Compute AWS Lambda Handles business logic and runs the TypeScript code. API Gateway AWS API Gateway Synchronous HTTP communication gateway for the entire Backend. Database NeonDB (Serverless PostgreSQL) Primary database for relational data. ORM Prisma Abstraction Layer between the code and the database. Security AWS SSM Parameter Store Secure storage for sensitive environment variables. DevOps GitHub Actions Automates the CI/CD pipeline. 2. Required Resources \u0026amp; Software To complete the workshop, users must have the following tools and accounts ready on their computer.\nA. Account Requirements AWS Account: Necessary for deploying Serverless services (Lambda, API Gateway, SSM). NeonDB Account: Necessary for creating and retrieving the connection string (DATABASE_URL) for the PostgreSQL database. GitHub Account: Necessary for storing the source code and setting up CI/CD (GitHub Actions). B. Local Software \u0026amp; Tools Node.js (v20+): Installed and accessible via the terminal. npm or yarn: Package manager. AWS CLI: Necessary to configure AWS access permissions from the local machine for the Serverless Framework. IDE (VS Code): Recommended development environment. Postman/Insomnia: Essential tool for testing API Endpoints (GET/POST). C. Project Setup Serverless Framework CLI: Must be installed globally (npm install -g serverless). AWS Credentials: Configure AWS access permissions (User/Role) on the local machine. "
},
{
	"uri": "http://localhost:1313/repo-name/1-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "SorcererXStreme: An AI-Powered Metaphysical Guidance Platform 1. Executive Summary The SorcererXStreme AI platform is a unified AI-driven spiritual guidance system designed to help users explore self-discovery through various Eastern and Western esoteric disciplines, including Astrology, Tarot, Numerology, and Eastern Horoscopes. The foundation of the system is the Retrieval-Augmented Generation (RAG) Core, which ensures all output is grounded in curated esoteric knowledge sources.\n2. Problem Statement What’s the Problem? Users currently face several limitations when exploring spiritual and esoteric knowledge:\nFragmented and Unverified Information: Information is scattered across the internet and often lacks credibility or proper cross-referencing. Difficulty in Cross-Discipline Comparison: Results are hard to compare between Eastern (e.g., Eastern Horoscope) and Western (e.g., Astrology) schools of thought. Lack of Personalization and Interaction: Most applications offer static readings, lacking the depth of personalized dialogue and contextual advice. Shallow Content: Many \u0026ldquo;fun\u0026rdquo; apps lack intellectual depth and robust knowledge. The Solution SorcererXStreme AI offers a unified, intuitive, and intelligent platform:\nDirect Interaction: Users chat directly with AI Chatbot, asking anything about their personality, fate, or relationships. RAG-Grounded Interpretations: The core RAG system guarantees that interpretations are based on verified esoteric data, ensuring accuracy and depth. Tiered User Experience: Free and VIP tiers optimize the user experience and create a revenue stream. Cost-Efficient Design: A modern, lightweight design rapidly deployed on a cost-optimized AWS serverless architecture. Benefits and Return on Investment Benefit Impact Value Data Reliability RAG reduces AI \u0026ldquo;hallucinations\u0026rdquo; and provides verifiable interpretations. High Trust \u0026amp; better user retention. Centralization Consolidates Eastern and Western mystical data in one platform. Unified Knowledge base for users. Monetization VIP subscription model unlocks advanced features. Stable Revenue stream and business viability. Operational Cost Serverless AWS architecture is used. Estimated $80–$90/month for MVP . 3. Solution Architecture The SorcererXStreme AI platform utilizes a robust, hybrid serverless architecture on AWS, meticulously designed to handle real-time user interactions, scheduled tasks, and autonomous monitoring.\nServices Used Layer Service Role and Relationship Edge \u0026amp; Auth Amplify, Cognito - Amplify handles frontend hosting and routing.\n- Cognito manages authentication. API \u0026amp; Routing API Gateway - The primary endpoint for all Backend Lambdas and AI Lambdas. Compute Layer AWS Lambda - Processes Business Logic and Async/Sync flows. Data \u0026amp; Integration DynamoDB, Parameter Store, NeonDB, Pinecone - NeonDB (external PostgreSQL) is the Primary DB.\n- DynamoDB for history/quick access.\n- Parameter Store for secure storage. AI/ML Bedrock, Lambda (Embeddings), S3 - Bedrock (LLM Model).\n- S3 stores raw RAG documents.\n- Lambda Embeddings creates vectors. Async \u0026amp; Monitoring EventBridge, SQS, SES, CloudWatch, SNS - Handles Asynchronous flows and Independent Monitoring activities. DevOps GitHub Actions, CloudFormation - GitHub Actions handles Build and Test process, and CloudFormation generated by the Serverless Framework is the main deployment tool. Workflow 1. Real-time API Interaction Flow (Synchronous Flow) This flow handles direct chat and interpretation requests from the user.\n(1) Request Reception: The User sends a request directly to the AWS Amplify Endpoint. (2) Routing \u0026amp; API: Amplify forwards the request to API Gateway. API Gateway authenticates the Cognito token and routes to the corresponding Lambda (SyncUser, Chatbot, etc.). (3) Data Processing: Lambda accesses Parameter Store for secrets and retrieves data from NeonDB, DynamoDB. (4) RAG and AI: The Chatbot Lambda executes the RAG flow: Uses Bedrock (Embedding Model) to create a vector for the question. Queries Pinecone (Vector Database) using that vector. Sends the RAG context to Bedrock (LLM) to generate the answer. (5) Response: Lambda returns the result to API Gateway -\u0026gt; Amplify -\u0026gt; User. 2. Automated Notification Flow (Asynchronous Flow) This flow is simplified and does not require RDS.\n(1) Trigger: EventBridge Scheduler triggers the TriggerReminder Lambda. (2) Data Query: Lambda queries DynamoDB or NeonDB to retrieve the list of subscribed users. (3) Distribution: Lambda generates content and sends emails via Amazon SES. 3. Deployment Flow (DevOps) (1) Code Commit: Developer pushes code to GitHub. (2) Build \u0026amp; Deploy: GitHub Actions triggers the Build, Test process and uses CloudFormation generated by the Serverless Framework to deploy Lambdas, API Gateway, and other resources to AWS. 5. Timeline \u0026amp; Milestones The SorcererXStreme project will be executed over a 9-week concentrated development period using an Agile-Iterative model to quickly deliver an MVP with key features.\nProject Timeline Iteration Duration Week Primary Focus Key Deliverables Iter 3: Redesign \u0026amp; RAG Prototype 3 Weeks 1 – 2 – 3 Platform Design \u0026amp; Documentation - SRS v2 and SDS v2, Proposal finalized.\n- AWS Architecture Diagram and cost estimate sheet.\n- RAG data collected and initial pipeline designed. Iter 4: Roles \u0026amp; VIP System 3 Weeks 4 – 5 – 6 Core Logic \u0026amp; Authorization Implementation - AWS Cognito integrated for user authentication.\n- Full Guest/Free/VIP role logic implemented and testable.\n- RAG data corpus built on S3. Iter 5: AWS Deployment \u0026amp; QA 3 Weeks 7 – 8 – 9 Cloud Deployment \u0026amp; Stabilization - System running stably on AWS.\n- Full end-to-end testing completed. AWS Cost and Performance Sheet finalized.\n- Ready for production environment. 6. Budget Estimate The project is estimated based on low usage for a Demo environment (approx. 5,000 requests/month).\nInfrastructure Costs Layer AWS Service Purpose Cost I. COMPUTE \u0026amp; API 1 AWS Lambda Backend Logic Processing (RAG, Compute) $0.00 2 Amazon API Gateway Synchronous Request Gateway $0.025 3 AWS Amplify Frontend Host (Next.js) $2.59 II. DATA \u0026amp; STORAGE 4 Amazon DynamoDB Chat History/Rate Limiting $0.89 5 Amazon S3 RAG Knowledge Base/Assets $0.03 III. AI \u0026amp; SECURITY 6 Amazon Bedrock Embedding \u0026amp; LLM/Content Generation $2.65 7 Amazon Cognito Authentication/User Roles $0.00 8 Parameter Store Store Master Keys $0.00 IV. ASYNC \u0026amp; MONITORING 9 EventBridge Scheduler Daily Horoscope Trigger $0.00 10 Amazon SES Email Delivery $0.48 11 Amazon CloudWatch Logs/Metrics/Alarms $2.4 12 Amazon SNS Alert Notifications $0.00 13 Cloudformation Deploy for dev $0.00 Link: Cost Estimation Sheet\nTotal Project Cost: $9.06/month 7. Risk Assessment Risk Matrix Risk Impact Probability Mitigation Strategy LLM Hallucination High Medium Implement RAG Fact Checker; use high-quality LLMs; ground answers in verified sources. Cost Overruns (LLM Calls) High Medium Set up AWS Budget Alerts; implement token control; use tiered LLM models (Free vs. VIP). RAG Retrieval Latency Medium Medium Optimize RAG indexing; optimize chunk size and embedding model choice. Security Breaches High Low Use Cognito for authentication and Secret Manager for credential handling. 8. Expected Outcomes Technical Improvements Real-time Accuracy: RAG integration significantly reduces AI \u0026ldquo;hallucinations,\u0026rdquo; enhancing the reliability of interpretations. Scalability: The serverless AWS architecture ensures automatic scaling to handle significant user traffic. Long-term Value Monetization: The VIP subscription model creates a clear, stable path for revenue generation. Data Foundation: A proprietary, verified esoteric knowledge base (RAG corpus) is established as a valuable, reusable asset. Future Expansion: The flexible AWS architecture (Lambda, Bedrock) is easily upgradable for mobile apps (React Native) or voice chat features. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.1-preparation/",
	"title": "Setting up Development Environment",
	"tags": [],
	"description": "",
	"content": "1. Prerequisites To develop modern web applications, you need the following standard tools:\nNode.js (LTS Version): Runtime environment for JavaScript/TypeScript. Git: Distributed version control system. IDE: Visual Studio Code (recommended). Recommended VS Code Extensions:\nESLint \u0026amp; Prettier: Automate formatting and linting. Tailwind CSS IntelliSense: Rapid Tailwind class suggestions. ES7+ React/Redux/React-Native snippets: Code faster with shortcuts. 2. Initialize Next.js Project We will use Next.js - the most popular React Framework today.\nRun initialization command:\nnpx create-next-app@latest my-serverless-app Detailed Configuration:\nTypeScript: Yes (Type safety) Tailwind CSS: Yes (Rapid styling) ESLint: Yes (Linting) App Router: Yes (Latest routing architecture) Import Alias: @/ (Cleaner imports) 3. Initialize AWS Amplify (Backend) This is a crucial step to integrate Serverless features (Auth, Data) into your project. Run the following command inside your project folder:\ncd my-serverless-app npm create amplify@latest When prompt to install, select Yes. Amplify will automatically create the amplify/ folder containing the Backend structure.\n4. Install AWS Libraries Install necessary SDK packages for Frontend to communicate with AWS:\nnpm install aws-amplify @aws-amplify/ui-react 5. Standard Project Structure After installation, your folder structure should look like this:\namplify/: Contains Backend code (auth.ts, data.ts). src/app: Contains Pages and Layouts (App Router). src/components: Contains reusable UI Components. amplify_outputs.json: Auto-generated config file (Do not edit). 6. Configure tsconfig.json (Best Practices) To ensure strict TypeScript coding, update tsconfig.json:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es5\u0026#34;, \u0026#34;lib\u0026#34;: [\u0026#34;dom\u0026#34;, \u0026#34;dom.iterable\u0026#34;, \u0026#34;esnext\u0026#34;], \u0026#34;allowJs\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;strict\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true, \u0026#34;noEmit\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;module\u0026#34;: \u0026#34;esnext\u0026#34;, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;resolveJsonModule\u0026#34;: true, \u0026#34;isolatedModules\u0026#34;: true, \u0026#34;jsx\u0026#34;: \u0026#34;preserve\u0026#34;, \u0026#34;incremental\u0026#34;: true, \u0026#34;plugins\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;next\u0026#34; } ], \u0026#34;paths\u0026#34;: { \u0026#34;@/*\u0026#34;: [\u0026#34;./src/*\u0026#34;] } }, \u0026#34;include\u0026#34;: [\u0026#34;next-env.d.ts\u0026#34;, \u0026#34;**/*.ts\u0026#34;, \u0026#34;**/*.tsx\u0026#34;, \u0026#34;.next/types/**/*.ts\u0026#34;], \u0026#34;exclude\u0026#34;: [\u0026#34;node_modules\u0026#34;] } 7. Install UI Libraries To create the \u0026ldquo;Mystical\u0026rdquo; UI:\nnpm install framer-motion lucide-react clsx tailwind-merge 8. Configure TailwindCSS Set up colors in tailwind.config.ts:\n// tailwind.config.ts import type { Config } from \u0026#34;tailwindcss\u0026#34;; const config: Config = { content: [ \u0026#34;./src/pages/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/components/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/app/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, ], theme: { extend: { colors: { primary: \u0026#34;#432c7a\u0026#34;, secondary: \u0026#34;#764ba2\u0026#34;, accent: \u0026#34;#ffd700\u0026#34;, background: \u0026#34;#1a0b2e\u0026#34;, }, backgroundImage: { \u0026#34;gradient-radial\u0026#34;: \u0026#34;radial-gradient(var(--tw-gradient-stops))\u0026#34;, }, }, }, plugins: [], }; export default config; My Experience Amplify Gen 2 vs Gen 1: If you used Amplify CLI (Gen 1) before with commands like amplify add auth, forget it! Gen 2 (what we are using) is Code-First. You define Backend using TypeScript (in amplify/ folder) instead of clicking through Console. It gives Frontend Devs much better control over infrastructure.\nVerification \u0026amp; Testing Test Case: Check Amplify Installation\nOpen package.json. Look inside dependencies. Expected Result: You should see \u0026quot;aws-amplify\u0026quot;: \u0026quot;^6.x.x\u0026quot; and \u0026quot;@aws-amplify/backend\u0026quot;: \u0026quot;^1.x.x\u0026quot;. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme: Building an AI-Powered Metaphysical Guidance Platform on AWS Overview SorcererXtreme AI is a pioneering metaphysical guidance platform that leverages AI and a robust AWS Serverless architecture to deliver personalized, grounded, and reliable readings across Astrology, Tarot, Horoscope and Numerology.\nContent Workshop overview Frontend Development Backend Development AI Development "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.2-ui-implementation/",
	"title": "Building Modern User Interface",
	"tags": [],
	"description": "",
	"content": "1. Layout Design (Responsive \u0026amp; Glassmorphism) Our goal is to create a mystical, \u0026ldquo;AI Sorcerer\u0026rdquo; interface like the Mockup below:\nUse CSS Grid and Flexbox from Tailwind to create flexible layouts.\n// src/app/layout.tsx export default function RootLayout({ children }: { children: React.ReactNode }) { return ( \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;body className=\u0026#34;bg-background text-white min-h-screen bg-[url(\u0026#39;/bg-stars.png\u0026#39;)] bg-cover\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-black/50\u0026#34; /\u0026gt; {/* Overlay */} \u0026lt;main className=\u0026#34;relative z-10 container mx-auto px-4 py-8 grid grid-cols-1 lg:grid-cols-12 gap-6\u0026#34;\u0026gt; {/* Sidebar takes 3 cols on Desktop */} \u0026lt;aside className=\u0026#34;lg:col-span-3 hidden lg:block\u0026#34;\u0026gt; {/* Sidebar Content */} \u0026lt;/aside\u0026gt; {/* Main Content takes 9 cols */} \u0026lt;section className=\u0026#34;lg:col-span-9\u0026#34;\u0026gt; {children} \u0026lt;/section\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ) } 2. Component: Horoscope Widget Create a Widget displaying daily horoscope with Glassmorphism effect.\n// src/components/HoroscopeWidget.tsx import { Star } from \u0026#39;lucide-react\u0026#39;; export default function HoroscopeWidget({ sign, prediction }: { sign: string, prediction: string }) { return ( \u0026lt;div className=\u0026#34;p-6 rounded-2xl bg-white/10 backdrop-blur-lg border border-white/20 hover:bg-white/20 transition-all duration-300 group\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3 mb-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;p-3 rounded-full bg-accent/20 text-accent group-hover:scale-110 transition-transform\u0026#34;\u0026gt; \u0026lt;Star size={24} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3 className=\u0026#34;text-xl font-bold\u0026#34;\u0026gt;{sign}\u0026lt;/h3\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p className=\u0026#34;text-gray-300 leading-relaxed\u0026#34;\u0026gt;{prediction}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); } 3. Component: Chat Interface Build chat interface with auto-scroll and dynamic message styling.\n// src/components/ChatBox.tsx import { useEffect, useRef } from \u0026#39;react\u0026#39;; import { clsx } from \u0026#39;clsx\u0026#39;; export default function ChatBox({ messages }: { messages: Message[] }) { const bottomRef = useRef\u0026lt;HTMLDivElement\u0026gt;(null); // Auto-scroll to bottom useEffect(() =\u0026gt; { bottomRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }, [messages]); return ( \u0026lt;div className=\u0026#34;flex flex-col h-[600px] bg-white/5 rounded-xl border border-white/10 overflow-hidden\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex-1 overflow-y-auto p-4 space-y-4\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;div key={idx} className={clsx( \u0026#34;max-w-[80%] p-3 rounded-lg\u0026#34;, msg.role === \u0026#39;user\u0026#39; ? \u0026#34;bg-primary self-end ml-auto\u0026#34; : \u0026#34;bg-white/10 self-start mr-auto\u0026#34; )}\u0026gt; {msg.content} \u0026lt;/div\u0026gt; ))} \u0026lt;div ref={bottomRef} /\u0026gt; \u0026lt;/div\u0026gt; {/* Input Area */} \u0026lt;/div\u0026gt; ); } 4. Animation with Framer Motion Add smooth entrance effects for UI elements.\nimport { motion } from \u0026#39;framer-motion\u0026#39;; const fadeIn = { hidden: { opacity: 0, y: 20 }, visible: { opacity: 1, y: 0 } }; \u0026lt;motion.div initial=\u0026#34;hidden\u0026#34; animate=\u0026#34;visible\u0026#34; variants={fadeIn} transition={{ duration: 0.5 }} \u0026gt; \u0026lt;HoroscopeWidget sign=\u0026#34;Leo\u0026#34; prediction=\u0026#34;Today is your lucky day!\u0026#34; /\u0026gt; \u0026lt;/motion.div\u0026gt; My Experience Mobile-First or Desktop-First? Initially, I designed for Desktop first, and when I opened it on mobile, the layout was broken. Lesson: Always use classes like hidden lg:block or grid-cols-1 lg:grid-cols-12 to prioritize Mobile layout first (default), then override for larger screens. TailwindCSS makes this incredibly easy!\nVerification \u0026amp; Testing Test Case 1: Responsive Design\nOpen browser on Desktop: See Sidebar on left, Chatbox on right. Press F12, switch to Mobile mode (iPhone 12/14). Expected Result: Sidebar hidden, Chatbox takes full width. Test Case 2: Hover Effects\nHover over HoroscopeWidget. Expected Result: Background brightens (bg-white/20), star icon scales up slightly (scale-110). "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/",
	"title": "Frontend Development",
	"tags": [],
	"description": "",
	"content": "Building Serverless Frontend with Next.js \u0026amp; AWS Amplify 1. Workshop Overview Welcome to the Building Modern Serverless Frontend workshop for the SorcererXtreme project.\nIn this project, the Frontend acts as the \u0026ldquo;face\u0026rdquo; of the application, where users directly interact with the mystical features. Our mission is to build a beautiful, smooth interface that communicates effectively with the backend AWS services.\n2. Frontend Architecture We will focus on the Client (Frontend) architecture and its Integration Points:\nFrontend Workflow: Hosting \u0026amp; Delivery: Next.js code is hosted and operated on AWS Amplify. Users access the web app via a global CDN (CloudFront) built into Amplify, ensuring lightning-fast load times. Authentication: When a user Logs In, the Frontend communicates directly with Amazon Cognito. Cognito returns a \u0026ldquo;Token\u0026rdquo; (like an access pass). API Interaction: For every request (like chatbot or Tarot reading), the Frontend sends this Token along with the request to Amazon API Gateway. Response: The Frontend receives JSON results from the API and renders the UI. The Frontend does not need to know what Database or AI model is behind the API; it only cares about the Input (Request) and Output (Response). 3. Frontend Tech Stack The \u0026ldquo;weaponry\u0026rdquo; of a Frontend Developer in this project:\nTechnology Role Why use it? Next.js (App Router) Framework Strong Server-Side Rendering (SSR) for SEO, powerful Router. AWS Amplify (Gen 2) Platform Provides Hosting, automated CI/CD, and fast Cloud connection libraries. Tailwind CSS Styling Rapid styling, easy customization for the mystical \u0026ldquo;Dark Mode\u0026rdquo;. Framer Motion Animation Create smooth motion effects (like 3D Tarot card flipping). Amplify UI Library Pre-built components for Login/Registration flows. Axios / Fetch HTTP Client Used to call API Gateway. 4. Estimated Time \u0026amp; Cost Item Details Time 2-3 hours per day Cost ~$9.06/month (Total project) 5. Workshop Content We will follow a standard Frontend development process:\nPreparation: Setup Next.js and Amplify. UI Implementation: Code Chat \u0026amp; Tarot UI with animations. Integration: Integrate Login (Cognito) and API calls (Gateway). CI/CD Pipeline: Push code to Git and auto-deploy to the Internet. Advanced: Custom Domain setup and SEO optimization. Backend Reference: Understanding RAG model. Cleanup: Cleaning up resources. Frontend Mindset: In Serverless architecture, Frontend is not just a \u0026ldquo;renderer\u0026rdquo;. It is also responsible for Security (keeping Tokens safe) and User Experience (handling Loading states while waiting for AI). Pay attention to these points during the workshop!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.2-set-up/",
	"title": "Setup environment",
	"tags": [],
	"description": "",
	"content": "Step 1: Project Initialization \u0026amp; Library Installation Create the project directory and install all the necessary core dependencies.\n# Create the main directory for the Backend mkdir my-serverless-backend \u0026amp;\u0026amp; cd my-serverless-backend # Initialize a Node.js package npm init -y # Install Core Dependencies: Express, Prisma Client, Serverless-HTTP, etc. npm install express cors dotenv @prisma/client axios serverless-http # Install Development Dependencies: TypeScript, Types for Node/Express, Prisma CLI, Serverless-Offline npm install -D typescript @types/node @types/express serverless-offline prisma serverless-dotenv-plugin Step 2: TypeScript \u0026amp; Directory Structure Setup Configure TypeScript and create the standard directory structure.\nInitialize tsconfig.json: npx tsc --init Edit tsconfig.json: Open the tsconfig.json file and adjust the following settings to ensure modern Node.js source code and compatibility with Lambda: \u0026quot;target\u0026quot;: \u0026quot;ES2020\u0026quot; (or newer) \u0026quot;module\u0026quot;: \u0026quot;commonjs\u0026quot; \u0026quot;outDir\u0026quot;: \u0026quot;./dist\u0026quot; (Output directory for compiled code) \u0026quot;rootDir\u0026quot;: \u0026quot;./src\u0026quot; (Source code directory) \u0026quot;esModuleInterop\u0026quot;: true \u0026quot;strict\u0026quot;: true Create Directory Structure: mkdir src src/routes src/services src/controllers Step 3: Code Restructuring (Express -\u0026gt; Lambda) Since Lambda does not \u0026ldquo;listen\u0026rdquo; on a standard port, we use serverless-http to wrap Express.\nFile src/app.ts (Core Express App):\nimport express from \u0026#39;express\u0026#39;; import cors from \u0026#39;cors\u0026#39;; import routes from \u0026#39;./routes/index\u0026#39;; // Change to index if you use routes/index.ts const app = express(); // 1. Basic Middlewares app.use(cors({ origin: process.env.FRONTEND_URL || \u0026#39;*\u0026#39; })); app.use(express.json()); // 2. API Routing (Main Endpoint will be /api/...) app.use(\u0026#39;/api\u0026#39;, routes); // IMPORTANT: Do not use app.listen(), eliminate traditional web server logic. export default app; File src/handler.ts (Lambda Bridge):\nimport serverless from \u0026#34;serverless-http\u0026#34;; import app from \u0026#34;./app\u0026#34;; // Export the main handler that AWS Lambda will call export const handler = serverless(app); Step 4: Configure Prisma \u0026amp; NeonDB Connection To connect to NeonDB and ensure the Prisma Client works in the AWS Lambda Linux environment, the binaryTargets must be configured.\nCreate Schema File \u0026amp; Configure binaryTargets: npx prisma init Open the file prisma/schema.prisma and add the configuration: generator client {\rprovider = \u0026#34;prisma-client-js\u0026#34;\r// native: For the dev machine (Mac/Win)\r// rhel-openssl-3.0.x: For AWS Lambda (Node 20)\rbinaryTargets = [\u0026#34;native\u0026#34;, \u0026#34;rhel-openssl-3.0.x\u0026#34;] }\rdatasource db {\rprovider = \u0026#34;postgresql\u0026#34;\rurl = env(\u0026#34;DATABASE_URL\u0026#34;)\r}\r// ... model definitions (User, Partner, Reminder, etc.) Create Local .env file: Create the .env file and paste your NeonDB connection string into it. # Get the PostgreSQL connection string from the Neon Console DATABASE_URL=\u0026#34;postgresql://[user]:[password]@[endpoint]/[dbname]?sslmode=require\u0026#34; Generate Prisma Client: Run the following command to create the Prisma Client and download the necessary binaries. npx prisma generate Complete: Your local environment is now ready. You can compile the code (npm run build) and run local tests with serverless-offline.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.3-configure-serverless/",
	"title": "Configure Serverless Framework",
	"tags": [],
	"description": "",
	"content": "This is the most crucial step, where we define the AWS infrastructure (IaC) and optimize the deployment package for Lambda.\n1. The serverless.yml File This configuration file includes sections for SSM Security, Build/Package Optimization, and necessary IAM Roles.\n# serverless.yml org: your_organization_name service: my-serverless-backend # Ensure service name matches the directory provider: name: aws runtime: nodejs20.x region: ap-southeast-1 timeout: 29 # Maximum allowed by API Gateway (keep as is) memorySize: 512 # Lambda memory configuration (recommended for Prisma) # --- VPC Network Configuration (MANDATORY for NeonDB) --- # If NeonDB requires a fixed IP or you use an internal RDS, # Lambda needs VPC configuration to connect externally or within the VPC. # (Assumed for NeonDB Public Access, but VPC needed if using Private Subnet) # vpc: # securityGroupIds: [sg-xxxxxxxx] # subnetIds: [subnet-xxxxxx] # --- Environment Variables (Fetched from SSM) --- environment: # Retrieve secrets from AWS SSM Parameter Store (Encrypted) DATABASE_URL: ${ssm:/my-app/${self:provider.stage}/database_url} JWT_SECRET: ${ssm:/my-app/${self:provider.stage}/jwt_secret} FRONTEND_URL: ${ssm:/my-app/${self:provider.stage}/frontend_url, \u0026#39;http://localhost:3000\u0026#39;} # Add local fallback PRISMA_CLI_BINARY_TARGETS: rhel-openssl-3.0.x # Crucial for Prisma # --- IAM Permissions (Required) --- # Add necessary permissions for Lambda to read SSM and access other services iam: role: statements: - Effect: \u0026#39;Allow\u0026#39; Action: - \u0026#39;ssm:GetParameter\u0026#39; Resource: \u0026#39;arn:aws:ssm:${self:provider.region}:*:parameter/my-app/${self:provider.stage}/*\u0026#39; # --- DDoS/Billing Protection (Keep as is) --- apiGateway: usagePlan: quota: limit: 5000000 period: MONTH throttle: burstLimit: 200 rateLimit: 100 # --- Deployment Package Size Optimization --- build: esbuild: bundle: true minify: true sourcemap: false # Explicitly specify modules to exclude from the main package external: - \u0026#39;aws-sdk\u0026#39; - \u0026#39;@prisma/client/runtime/library\u0026#39; package: individually: true patterns: - \u0026#39;src/handler.js\u0026#39; - \u0026#39;src/app.js\u0026#39; - \u0026#39;src/**/*.js\u0026#39; - \u0026#39;dist/**/*.js\u0026#39; # Ensure compiled JS files are packaged - \u0026#39;package.json\u0026#39; - \u0026#39;node_modules/**\u0026#39; # --- Define Prisma Binary Files (Extremely important) --- # Only include the necessary Linux binary file to keep package \u0026lt; 250MB - \u0026#39;node_modules/.prisma/client/libquery_engine-rhel-openssl-3.0.x.so.node\u0026#39; - \u0026#39;node_modules/.prisma/client/schema.prisma\u0026#39; - \u0026#39;!./**\u0026#39; # Remove all unnecessary files after defining required patterns above - \u0026#39;!node_modules/aws-sdk/**\u0026#39; # Reduce size by excluding SDK already available in Lambda plugins: - serverless-offline - serverless-dotenv-plugin functions: api: handler: src/handler.handler events: - http: { path: /, method: ANY } - http: { path: /{proxy+}, method: ANY } 2. Required Steps A. Store Secrets in AWS SSM (Security) Before deployment, you must store sensitive values in the AWS SSM Parameter Store so the Serverless Framework can read them.\n# Command to store DATABASE_URL (SecureString) aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:password@endpoint...\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # Repeat for JWT_SECRET and FRONTEND_URL B. Run Local Testing Use the serverless-offline plugin to run the API locally, connecting directly to NeonDB via the .env file.\n# Run local API on port 3000 (default) sls offline start C. First-time Deployment Once the code has been locally tested, you are ready to deploy the entire infrastructure to AWS.\n# Deploy all resources (Lambda, API Gateway, IAM) sls deploy "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.3-integration/",
	"title": "Integrating API Gateway &amp; Authentication",
	"tags": [],
	"description": "",
	"content": "1. Authentication Flow Before coding, let\u0026rsquo;s understand how Frontend communicates with Cognito and API Gateway:\nUser enters User/Pass. Cognito returns JWT Token (ID Token, Access Token). Frontend sends Request with Token in Authorization Header. API Gateway verifies Token. If valid -\u0026gt; Forward to Lambda. 2. Configure AWS Amplify Install aws-amplify library to connect Frontend with AWS services:\nnpm install aws-amplify @aws-amplify/ui-react Configure in src/app/layout.tsx:\n\u0026#39;use client\u0026#39;; import { Amplify } from \u0026#39;aws-amplify\u0026#39;; import config from \u0026#39;@/amplifyconfiguration.json\u0026#39;; Amplify.configure(config); 3. Integrate Amazon Cognito (Authentication) Use Authenticator component to create a secure login/signup flow:\nimport { Authenticator } from \u0026#39;@aws-amplify/ui-react\u0026#39;; export default function LoginPage() { return ( \u0026lt;Authenticator\u0026gt; {({ signOut, user }) =\u0026gt; ( \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;Welcome, {user?.username}\u0026lt;/h1\u0026gt; \u0026lt;button onClick={signOut}\u0026gt;Sign Out\u0026lt;/button\u0026gt; \u0026lt;/main\u0026gt; )} \u0026lt;/Authenticator\u0026gt; ); } 4. Custom Hook: useAuth (Best Practices) Instead of calling fetchAuthSession everywhere, create a Custom Hook to reuse authentication logic and token retrieval.\n// src/hooks/useAuth.ts import { fetchAuthSession } from \u0026#39;aws-amplify/auth\u0026#39;; import { useState, useEffect } from \u0026#39;react\u0026#39;; export function useAuth() { const [token, setToken] = useState\u0026lt;string | null\u0026gt;(null); useEffect(() =\u0026gt; { const getToken = async () =\u0026gt; { try { const session = await fetchAuthSession(); setToken(session.tokens?.idToken?.toString() || null); } catch (err) { console.error(\u0026#34;Error fetching auth session\u0026#34;, err); } }; getToken(); }, []); return { token }; } 5. API Call with Error Handling Handle errors professionally using try/catch/finally and display feedback to users.\n// src/services/api.ts export const chatWithAI = async (message: string, token: string) =\u0026gt; { try { const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/chat`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: `Bearer ${token}` }, body: JSON.stringify({ message }) }); if (!response.ok) { if (response.status === 401) throw new Error(\u0026#34;Session expired\u0026#34;); if (response.status === 429) throw new Error(\u0026#34;Too many requests\u0026#34;); throw new Error(\u0026#34;System error\u0026#34;); } return await response.json(); } catch (error) { console.error(\u0026#34;API Error:\u0026#34;, error); throw error; // Throw error for UI to handle } }; My Experience Never expose API Keys! Once I accidentally committed a .env file containing API Keys to GitHub. AWS sent a warning email immediately. Solution: Always add .env to .gitignore. With Amplify, amplifyconfiguration.json is safe to be public as it only contains Resource IDs (like User Pool ID), not Secret Keys.\nVerification \u0026amp; Testing Test Case 1: Successful Login\nGo to Login page, enter created User/Pass. Click Sign In. Expected Result: Redirect to main page, display \u0026ldquo;Welcome, [Username]\u0026rdquo;. Test Case 2: Token Check\nOpen DevTools (F12) -\u0026gt; Network Tab. Send a Chat message. Find request sent to API Gateway. Check Header: Must have Authorization: Bearer eyJra... line (JWT Token). "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.4-deployment/",
	"title": "Setting up CI/CD Pipeline with AWS Amplify",
	"tags": [],
	"description": "",
	"content": "1. CI/CD Process We will set up a fully automated process as follows:\nSource: Developer pushes code to GitHub. Build: Amplify automatically detects changes, installs dependencies, and builds Next.js. Deploy: Pushes built code to global Hosting system. Verify: Checks website health (Health Check). 2. Connect Repository Push code to Git Repository (GitHub/GitLab/CodeCommit). In AWS Amplify Console, select Host web app. Connect to the Repository containing Frontend source code. 3. Build Configuration (Build Specification) Amplify uses amplify.yml to define build steps.\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: .next files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* 4. Environment Variables Management Never hard-code sensitive or environment-specific values (like API URLs) in your code. Use Environment Variables in Amplify.\nGo to App settings \u0026gt; Environment variables. Add variable: Key: NEXT_PUBLIC_API_URL Value: https://xyz.execute-api.us-east-1.amazonaws.com/prod Access in Next.js code via process.env.NEXT_PUBLIC_API_URL. Note: Variables starting with NEXT_PUBLIC_ will be embedded into the Frontend code by Next.js at build time.\n5. Branch Previews (Pull Request Previews) This feature is incredibly useful for team collaboration.\nWhen you create a Pull Request (PR) on GitHub, Amplify automatically creates a temporary Preview environment (with a unique URL). Team Leaders can visit that URL to review new features before Merging into the main branch. After Merging, the Preview environment is automatically deleted. To enable: Go to App settings \u0026gt; Previews \u0026gt; Enable previews.\nMy Experience Build Error on Linux vs Windows My machine uses Windows (case-insensitive), but Amplify runs Linux (case-sensitive). Once I imported Component.tsx but the file was named component.tsx. It ran fine locally, but failed on Amplify with \u0026ldquo;File not found\u0026rdquo;. Lesson: Always name files consistently (PascalCase for Components, camelCase for utils) and double-check when renaming files.\nVerification \u0026amp; Testing Test Case 1: Trigger Build\nEdit a small text line in page.tsx. Commit and Push to GitHub: git push origin main. Go to Amplify Console. Expected Result: See status change to \u0026ldquo;Provisioning\u0026rdquo; -\u0026gt; \u0026ldquo;Building\u0026rdquo; -\u0026gt; \u0026ldquo;Deploying\u0026rdquo;. Test Case 2: Check Logs\nClick on the running build. Open Frontend tab. Expected Result: See green logs (Success) in steps. 2024-05-20T10:00:00.000Z [INFO]: # Executing command: npm run build\r...\r2024-05-20T10:01:00.000Z [INFO]: Compiled successfully "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.4-aws-ssm/",
	"title": "Storing Sensitive Environment Variables",
	"tags": [],
	"description": "",
	"content": "Storing sensitive environment variables (DATABASE_URL, JWT_SECRET) in AWS Systems Manager (SSM) Parameter Store is the security standard for Serverless applications.\n1. Preparation In your serverless.yml, you used the variable ${self:provider.stage} (defaulting to dev if not specified). To be consistent with that configuration, we should define the Key in the format /my-app/\u0026lt;stage\u0026gt;/\u0026lt;key\u0026gt;.\n2. Run CLI Commands to Store Keys You need to run these commands via the AWS CLI after configuring access permissions.\n# NOTE: Replace \u0026lt;YOUR_VALUE\u0026gt; with your actual value. # We will use the default stage \u0026#34;dev\u0026#34; for the workshop environment. # --- 1. Store NeonDB Connection String --- # Use \u0026#34;SecureString\u0026#34; to encrypt the data. aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:pass@ep-xyz.aws.neon.tech/neondb?sslmode=require\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 2. Store JWT Secret --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/jwt_secret\u0026#34; \\ --value \u0026#34;a_very_long_and_secure_jwt_key_314159\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 3. Store Frontend URL (If necessary for CORS) --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/frontend_url\u0026#34; \\ --value \u0026#34;https://your-amplify-app.amplifyapp.com\u0026#34; \\ --type \u0026#34;String\u0026#34; \\ --overwrite 3. Verification After storing, you can run the following command to check if the Parameter has been saved correctly and if the Serverless Framework can access it:\n# Command to check and decrypt the value (requires SSM Read permission) aws ssm get-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --with-decryption Important Note: In serverless.yml, you defined: DATABASE_URL: ${ssm:/my-app/prod/database_url}. To synchronize with the command above, you need to change prod to dev in serverless.yml if you are deploying with the default stage, or run the CLI commands with the prod stage if you wish to keep the serverless.yml file as is.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.5-cicd/",
	"title": "Automation CI/CD",
	"tags": [],
	"description": "",
	"content": "The objective of this section is to establish a process so that every time code is pushed to the main branch, your entire Serverless architecture will be automatically Built, Tested, and Deployed to AWS without manual intervention from a local machine.\n1. Preparing Secrets on the GitHub Repository This is the most crucial step for granting GitHub Actions access to AWS.\nSecret Name Role Notes and Action AWS_ACCESS_KEY_ID AWS Access Key. VERY IMPORTANT: Use an IAM User with minimum necessary permissions (not Admin) to only allow creation/updating of Lambda, API Gateway resources, and reading SSM. AWS_SECRET_ACCESS_KEY Corresponding Secret Key. (Keep as is) SERVERLESS_ACCESS_KEY Serverless Dashboard Key. Only needed if you use Serverless Dashboard management features. If not, this can be omitted. 2. Workflow File .github/workflows/deploy.yml This file defines the steps that GitHub Actions will execute. We will add the Build step and omit environment variables unnecessary for the deploy command.\nname: Deploy Backend CI/CD via Serverless Framework on: push: branches: [ main ] # Triggers when pushing to the main branch workflow_dispatch: # Allows manual triggering from the GitHub UI jobs: deploy: runs-on: ubuntu-latest # Grant permissions for GitHub Actions permissions: id-token: write contents: read steps: - uses: actions/checkout@v4 # 1. Get source code - name: Setup Node 20 uses: actions/setup-node@v3 with: node-version: \u0026#39;20\u0026#39; cache: \u0026#39;npm\u0026#39; # Enable cache to speed up installation - name: Install Dependencies run: npm ci # Use npm ci to ensure consistency (from package-lock.json) # --- PRISMA \u0026amp; CODE BUILD CONFIGURATION --- - name: Generate Prisma Client (download Linux binary) # Download the rhel-openssl-3.0.x binary (necessary for Lambda) run: npx prisma generate - name: Build TypeScript Code (tsc -\u0026gt; dist) # Run the compilation command (assumes a \u0026#34;build\u0026#34;: \u0026#34;tsc\u0026#34; script in package.json) run: npm run build # --- AWS CONFIGURATION --- - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-southeast-1 # --- FINAL DEPLOYMENT --- - name: Deploy Serverless # Use npx to call the serverless CLI run: npx serverless deploy env: # Explicitly specify the deployment stage. SLS_STAGE: dev # [Note]: The DATABASE_URL variable is already handled by placeholder or SSM DATABASE_URL: \u0026#34;placeholder\u0026#34; 3. CI/CD Workflow Summary Code Commit: You develop code and perform a git push to GitHub. Trigger: GitHub Actions automatically triggers the deploy.yml workflow. Build \u0026amp; Package: The Ubuntu Runner installs dependencies, runs npx prisma generate (downloading the Linux binary), and compiles the code. AWS Authentication: aws-actions/configure-aws-credentials logs into AWS using your Secret Keys. Provisioning: The npx serverless deploy command reads serverless.yml, connects to SSM Parameter Store to fetch secret keys, and instructs CloudFormation to build/update the entire Lambda and API Gateway infrastructure. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.5-advanced-deployment/",
	"title": "Optimization &amp; Advanced Deployment",
	"tags": [],
	"description": "",
	"content": "After successfully deploying SorcererXtreme to the Internet, our work is not done. To make the product truly \u0026ldquo;Production-Ready\u0026rdquo;, we need to perform advanced tuning.\n1. Custom Domain By default, AWS Amplify provides a long and hard-to-remember URL (e.g., main.d12345.amplifyapp.com). Setting up a custom domain not only makes the app look professional but also improves trust.\nProcess:\nPurchase Domain: You can buy directly on Amazon Route 53 or other providers (Namecheap, GoDaddy). Configure in Amplify: Go to App settings \u0026gt; Domain management. Click Add domain and enter your domain (e.g., sorcererxtreme.vn). DNS Verification: If on Route 53: Amplify auto-configures (Zero-config). If external: Amplify provides a CNAME record for you to add to your provider\u0026rsquo;s DNS manager. SSL/TLS: Amplify automatically issues and manages free SSL certificates, ensuring the green padlock (HTTPS). 2. Environment Variables Management During development, we often use .env.local files to store sensitive Keys. However, these files are not pushed to Git when deploying.\nWhy important?\nSecurity: Hides API keys (like API Gateway Endpoint, Cognito User Pool ID) from public source code. Flexibility: Easily switch configs between Staging and Production environments without changing code. Setup:\nAccess Amplify Console \u0026gt; Select App \u0026gt; Environment variables. Enter corresponding Keys (e.g., NEXT_PUBLIC_API_URL, NEXT_PUBLIC_USER_POOL_ID). Trigger the Build process again for changes to take effect. 3. SEO Optimization for Next.js (Search Engine Optimization) For a B2C user-facing app like Tarot reading, appearing on Google is vital. Next.js (App Router) strongly supports SEO via the Metadata API.\nDynamic Metadata: Instead of static titles, create dynamic titles based on the Tarot card the user draws:\n// app/tarot/[cardId]/page.tsx export async function generateMetadata({ params }) { const card = await getTarotCard(params.cardId); return { title: `Meaning of ${card.name} | SorcererXtreme`, description: `Discover the cosmic message from ${card.name}...`, openGraph: { images: [card.imageUrl], // Image shown when shared on Facebook }, } } 4. Monitoring \u0026amp; Analytics You cannot improve what you do not measure. Use the Monitoring Tab in Amplify to track:\nIncoming Traffic: Number of visitors in real-time. Data Transfer: Bandwidth usage. Error Rate (4XX/5XX): Detect immediately if APIs fail or links break. Access Logs: Download access logs to analyze user behavior (origin country, browser used). Front-end Tip: Always check your Lighthouse score (in Chrome DevTools) after every deploy. A beautiful app that loads slowly will lose users instantly. Optimize images (use WebP/AVIF format) and lazy-load heavy components.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.6-backend-architecture/",
	"title": "Backend Reference Architecture (RAG &amp; Database)",
	"tags": [],
	"description": "",
	"content": "As a modern Frontend Developer, the line between Frontend and Backend is blurring. You don\u0026rsquo;t need to write SQL or manage servers, but you usually must understand the data flow to integrate effectively.\nThe SorcererXtreme system uses an advanced RAG (Retrieval-Augmented Generation) architecture. Let\u0026rsquo;s dissect what happens after you call fetch('/api/chat').\n1. RAG Flow: Why is the AI accurate? If we simply send the question to ChatGPT, it might \u0026ldquo;hallucinate\u0026rdquo; based on old training data. To make the AI act like a \u0026ldquo;Wise Sorcerer\u0026rdquo; knowledgeable in Tarot, we use a 4-step process:\nRetrieval: When User asks \u0026ldquo;What does The Fool card mean?\u0026rdquo;, the question is converted into a vector (numbers). The system searches Pinecone (Vector DB) for Tarot book passages with the most similar meaning. Augmentation: The system combines the original question + found content from Pinecone into a complete Prompt. Prompt: \u0026ldquo;Based on the following knowledge [book excerpt\u0026hellip;], answer the question: What does The Fool card mean?\u0026rdquo; Generation: Send this Prompt to Amazon Bedrock (hosting Claude 3 or Titan models). AI answers based strictly on the provided text, avoiding fabrication. Response: Frontend receives the final answer and renders it. 2. \u0026ldquo;Polyglot Persistence\u0026rdquo; Strategy A large application never uses just one type of Database. We use the right tool for the right job:\nA. NeonDB (Serverless PostgreSQL)\nType: Relational. Data: User Profiles, VIP tiers, Payment transactions. Why: Financial data requires the highest integrity (ACID). SQL is the #1 choice. B. Amazon DynamoDB\nType: NoSQL (Key-Value). Data: Chat History, Activity Logs. Why: Chat generates millions of records. DynamoDB offers limitless scale with single-digit millisecond latency. C. Pinecone\nType: Vector Database. Data: Tarot/Astrology Knowledge (encoded as Vectors). Why: SQL or NoSQL cannot search by \u0026ldquo;meaning\u0026rdquo; (semantic search). Only a Vector DB understands that \u0026ldquo;King of Coins\u0026rdquo; and \u0026ldquo;King of Pentacles\u0026rdquo; are related. 3. Security: The \u0026ldquo;Fortress\u0026rdquo; Model Your Frontend (sorcererxtreme.vn) is public land. But the Backend is a sanctuary.\nNo Exposed Databases: NeonDB or Pinecone never open ports to the Internet. API Gateway as the Guard: All requests must pass through API Gateway. It checks the \u0026ldquo;ID Badge\u0026rdquo; (Cognito Token). If missing or expired -\u0026gt; Block immediately (401 Unauthorized). Lambda as the Courier: Only Lambda functions (residing in a private VPC) hold the Secret Keys to unlock the Database doors. Takeaway: When debugging \u0026ldquo;why did the AI give a wrong answer?\u0026rdquo;, a Frontend Dev can guess: \u0026ldquo;Ah, maybe the Retrieval step in Pinecone found the wrong context\u0026rdquo;, instead of blaming the AI Model. Understanding the system helps you fix bugs faster!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.6-microservice/",
	"title": "Connect microservices",
	"tags": [],
	"description": "",
	"content": "In a Serverless architecture, when one Lambda function (e.g., Chatbot) needs to call another service (MetaphysicalAPI), the best approach is to use an HTTP request through that service\u0026rsquo;s API Gateway.\n1. Principles of API Calling Endpoint: Always call through the public API Gateway Endpoint (or an internal one if both Lambdas are within a VPC and use a Private API Gateway). Library: Use a familiar HTTP library like axios to manage the request and client-side timeout. Security: Use IAM Roles or API Keys to authenticate calls between services if the API Gateway is not entirely public. 2. Timeout Management Managing the timeout is the most critical factor to prevent 504 Gateway Timeout errors.\n[Image of AWS API Gateway 29-second timeout limit diagram]\nAPI Gateway Limit: The API Gateway has a hard timeout limit of 29 seconds. Any request lasting longer than 29 seconds is cut off and returns a 504 error. Lambda Limit: Lambda can run up to 15 minutes, but it is constrained by the API Gateway. Therefore, the Lambda timeout must be set less than 29 seconds (e.g., 25 seconds). Configuration Location Recommended Value Rationale API Gateway Timeout serverless.yml (Provider level) 29 seconds AWS maximum hard limit. Lambda Timeout (Receiver) serverless.yml (Function level) 25 seconds Must be less than 29s so Lambda can return an error before the API Gateway cuts the connection. Client Timeout (axios) TypeScript Code 25,000 ms (25 seconds) Ensures the client cuts the connection before Lambda times out, allowing for cleaner client-side error handling. 3. Lambda Performance Optimization If your AI Service performs heavy tasks (like RAG, astronomical calculations), optimizing processing speed is mandatory.\nIncrease Memory (RAM): Increase the memorySize of the AI Service Lambda to a higher level (e.g., 1536 MB or 3008 MB). On AWS, increasing RAM also increases CPU and Network Bandwidth, significantly speeding up heavy processing and reducing the likelihood of 504 errors. Code Configuration: Configure timeout: 25 in serverless.yml for both the Backend and the AI Service. 4. Code Example Here is the AI calling function, modified to handle exceptions clearly and adhere to the Timeout principle.\nimport axios from \u0026#39;axios\u0026#39;; // Assume AI_SERVICE_URL is retrieved from AWS SSM Parameter Store and injected into ENV const AI_SERVICE_URL = process.env.AI_SERVICE_URL; const CLIENT_TIMEOUT_MS = 25000; // 25 seconds (Below the 29s API Gateway limit) /** * Calls the AI service\u0026#39;s API Gateway to receive a response. * @param prompt The request data sent to the AI. */ export const callAI = async (prompt: string, userId: string) =\u0026gt; { if (!AI_SERVICE_URL) { throw new Error(\u0026#34;AI Service URL is not configured.\u0026#34;); } try { const res = await axios.post(AI_SERVICE_URL, { prompt, userId // Send user ID so the AI Service can log or handle VIPs }, { timeout: CLIENT_TIMEOUT_MS // Set client timeout }); // Handle successful response return res.data; } catch (error) { if (axios.isAxiosError(error) \u0026amp;\u0026amp; error.code === \u0026#39;ECONNABORTED\u0026#39;) { // Client-side Timeout error console.error(\u0026#34;AI Timeout Error: Request took too long.\u0026#34;); throw new Error(\u0026#34;AI Service timed out (504). Please try again.\u0026#34;); } // Other errors (e.g., 4xx, 5xx from AI Service) console.error(\u0026#34;AI Service Error:\u0026#34;, error.message); throw new Error(\u0026#34;AI Service unavailable or returned an error.\u0026#34;); } }; "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "After completing the Workshop, cleaning up is mandatory to avoid unexpected AWS charges. Since we use Serverless Services (Amplify, Lambda, Bedrock), cleanup is straightforward but requires attention.\n1. Delete AWS Amplify App This is the most critical step. Deleting the App on Amplify automatically cleans up 80% of related resources (S3 Hosting, CloudFront, CI/CD Pipeline).\nAccess AWS Amplify Console. Select the SorcererXtreme app. Click Actions tab (top right) -\u0026gt; Select Delete app. Type the confirmation phrase (usually delete) and click Confirm. 2. Cleanup Database \u0026amp; External Services Since NeonDB and Pinecone are 3rd party services (not included in Amplify Delete), you must delete them manually:\nNeonDB: Log in to Neon Console. Go to Project Settings -\u0026gt; Delete Project. Pinecone: Log in to Pinecone Console. Delete the Index (e.g., tarot-knowledge-base) to stop vector storage billing. 3. Manual AWS Resource Cleanup (If created separately) If you created extra resources outside of Amplify during the workshop, check:\nAmazon Bedrock: Bedrock is billed On-demand per request, so you don\u0026rsquo;t need to \u0026ldquo;delete\u0026rdquo; Models. However, if you created a custom Knowledge Base, delete it. Amazon Cognito: Check if the User Pool is gone (Amplify usually deletes it). Parameter Store: Go to AWS Systems Manager \u0026gt; Parameter Store \u0026gt; Delete keys like /sorcerer/neon_db_url, /sorcerer/pinecone_api_key. 4. Final Switch (Billing Dashboard) To be 100% sure:\nAccess AWS Billing Dashboard. Check the \u0026ldquo;Bills\u0026rdquo; section. Wait 24h for the system to update and ensure no new charges are appearing from unknown services. Conclusion Congratulations on reaching the end of the journey!\nYou have successfully built SorcererXtreme - an application blending Frontend artistry (Next.js), Cloud power (AWS Amplify), and Artificial Intelligence (Bedrock RAG).\nSee you in advanced Workshops!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.7-email/",
	"title": "Setup Email",
	"tags": [],
	"description": "",
	"content": "To ensure the highest delivery reliability for project emails and prevent them from being marked as Spam by email service providers (like Gmail, Outlook), domain security configuration is mandatory.\n1. Basic Configuration and Identity Security Step Action Purpose 1. Domain Purchase Use a private domain (.com, .xyz) instead of a shared one. Reputation: Establish a separate email sending reputation, avoiding impact from other bad senders. 2. Verify Domain in AWS SES Access SES -\u0026gt; Verified Identities -\u0026gt; Create Identity (Select Domain). Proof of Ownership: Allows AWS SES to manage email sending on behalf of your domain. 3. Configure DNS (Email Security) Add CNAME (DKIM), TXT (SPF), and TXT (DMARC) records. Crucial: These records verify the sending source (DKIM/SPF) and instruct the receiving server how to handle invalid emails (DMARC), preventing emails from falling into Spam. Details of Mandatory DNS Records: DKIM (CNAME): Add the 3 CNAME records provided by AWS SES. Purpose: Digital Signature. SPF (TXT): Add a TXT record with the content: v=spf1 include:amazonses.com ~all. Purpose: Designates AWS SES as an authorized server to send mail. DMARC (TXT): Add a TXT record (usually as _dmarc) with the content: v=DMARC1; p=none;. Purpose: Establishes reporting and handling policies for fraudulent emails. 2. Setting up the Sending Flow and Exiting Sandbox After the domain has been verified (Verification Status: Verified), you need to set up the operational flow and request actual sending permission.\nStep Interacting Service Action 1. Activate Async Flow EventBridge Scheduler -\u0026gt; Lambda (TriggerReminder) The asynchronous flow starts according to a defined schedule. 2. Send SES Request Lambda (TriggerReminder) -\u0026gt; Amazon SES API The Lambda function (with the granted IAM Role) directly calls the SES API (e.g., SendEmailCommand) to send personalized emails to each user. 3. Request Production Access AWS Support Center Submit a ticket requesting AWS to upgrade your account from Sandbox mode so you can send emails to any unverified address. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.8-summary/",
	"title": "Development workflow",
	"tags": [],
	"description": "",
	"content": "The workflow is designed to leverage the speed of local development and the safety and automation of the Serverless architecture.\nStep Activity Environment Role and Notes 1. Code Development Code logic, API modifications (TypeScript/Express). Local Machine Use VS Code and Node.js libraries. 2. Local Testing Test synchronous APIs. sls offline start Simulates the Lambda/API Gateway environment. Connects directly to NeonDB via the local .env file. 3. Database Schema Update If Schema is modified (schema.prisma). npx prisma migrate dev Creates Migration and applies changes. 4. Commit \u0026amp; Push Code Commit code and push to the repository. git push origin main Triggers the automated CI/CD flow. 5. Automated CI/CD Deployment to the Cloud. GitHub Actions Automated: Build -\u0026gt; Prisma Generate -\u0026gt; AWS/SSM Login (fetch Keys) -\u0026gt; Deploy to AWS Lambda. 6. Error Monitoring Check Real-time Logs. sls logs -f api -t Use the Serverless Framework command to immediately view CloudWatch Logs and debug errors in the Production/Staging environment. Important Notes on Prisma: Use migrate dev: Instead of db push (which is only for non-production/test), you should use npx prisma migrate dev to create a history of changes (migration files) and apply them to NeonDB. Generate on CI/CD: Running npx prisma generate in GitHub Actions is mandatory to download the necessary binaries (rhel-openssl-3.0.x) for the AWS Lambda Linux environment. "
},
{
	"uri": "http://localhost:1313/repo-name/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/repo-name/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]