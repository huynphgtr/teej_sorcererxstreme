[
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.1-architecture/",
	"title": "Architecture &amp; Tech Stack",
	"tags": [],
	"description": "",
	"content": "The system is built upon a Serverless architecture on AWS to optimize operational costs and scalability.\n1. Core Technology Stack Component Selected Technology Rationale Language Python 3.x Specialized AI Ecosystem. The optimal deployment environment for LLMs, Embeddings, and RAG workflows. Compute AWS Lambda Event-Driven Serverless Architecture. Optimizes operational costs (pay-per-request) and offers seamless integration with the AWS ecosystem (S3, API Gateway). Raw Storage Amazon S3 Durable \u0026amp; Cost-Effective Storage. Securely stores raw data, integrated directly with Lambda automated processing triggers. Vector DB Pinecone Managed Service. Specialized for Vector Search, offering high query speeds, low latency, and zero infrastructure management. Meta DB Amazon DynamoDB Millisecond Latency. Optimized for Exact Match queries and storing conversation history with instant response speeds. AI Model Amazon Bedrock Unified API. Access to diverse Foundation Models via a single gateway, ensuring absolute data privacy and security. CI/CD GitHub Actions Automation. Automates the Testing and Deployment process to Lambda immediately upon code push. 2. Resources \u0026amp; Environment Setup Before proceeding with the detailed implementation, ensure the following resources and access rights are established:\nAWS Account \u0026amp; Region:\nAn active AWS account with billing enabled. Third-Party Services:\nPinecone: Create an account and generate an API Key (Serverless Index). GitHub: Configure GitHub Secrets to securely store credentials (AWS_ACCESS_KEY, PINECONE_API_KEY) for CI/CD pipelines. Local Environment:\nInstall AWS CLI v2 and run aws configure. Install Python 3.9+ and SAM CLI (Serverless Application Model) for building and deploying Lambda functions. "
},
{
	"uri": "http://localhost:1313/repo-name/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Group Internship Details Project Information Field Detail Group name TEEJ_SorcererXStreme University FPT University - Ho Chi Minh Campus Internship company Amazon Web Services Vietnam Co., Ltd. Internship position FCJ Workforce Program Intern Internship duration From 08/09/2025 to 24/12/2025 Team Member Photo Role Full Name Major Contact Leader Tran Phuong Huyen Software Engineering tranphuonghuyen2005@gmail.com AI Nguyen Lam Anh Artifical Intelligent nguyenla110505@gmail.com AI Nguyen Van Linh Artifical Intelligent nguyenvanlinh.1710.it@gmail.com SE Bui Nguyen Tan Khang Software Engineering tankhang6a6@gmail.com "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme AI: Building an AI-Powered Metaphysical Guidance Platform on AWS The core purpose of this project, for a developer workshop, is to demonstrate how to build a scalable, cost-optimized, multi-faceted application capable of handling complex data flows entirely within the cloud environment.\nProblem Solved \u0026amp; Technical Value The Challenge: Building a platform that combines the need for precise computation with the linguistic creativity of AI, while ensuring all content is verifiable and grounded. Traditional server-based solutions often struggle with the dynamic scaling required for such varied workloads. The Technical Solution: We solve this by implementing a Retrieval-Augmented Generation (RAG) Core utilizing Amazon Bedrock and Pinecone. This design allows the AI to produce verified answers based on a specialized knowledge base, transforming speculative guidance into actionable insights. Key Technical Highlights This project serves as an essential case study for integrating the following critical AWS services:\nServerless Compute: We utilize AWS Lambda as the entire Backend, eliminating server management overhead and significantly optimizing costs. Modern Deployment (Frontend Hosting): Deploying the Next.js application on AWS Amplify provides streamlined CI/CD and hosting for the Frontend. Durable Asynchronous Flow (Async): We constructed a reliable automated reminder system using EventBridge -\u0026gt; Lambda -\u0026gt; SES. This pattern ensures robust, scalable bulk delivery without overloading the core API. Data Persistence and Vectors: We manage complex relational data externally using NeonDB (Serverless PostgreSQL) while utilizing a specialized Vector Database (Pinecone) for the high-speed RAG retrieval layer. Security and DevOps: AWS Parameter Store manages all sensitive keys, and the entire infrastructure is deployed using the Serverless Framework driven by GitHub Actions (CI/CD). Best Practices Learned Attendees will learn how to implement a fully Serverless Microservices architecture, address challenges like external database connectivity, splitting synchronous/asynchronous workloads, and building a cost-effective RAG Core solution.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.1-prepare/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "1. Technologies Used Category Technology Detail \u0026amp; Role Language TypeScript (Node.js 20) Primary language, providing Type Safety. Backend Core Express.js + serverless-http Familiar API Framework, \u0026ldquo;wrapped\u0026rdquo; to run on Lambda. Infrastructure (IaC) Serverless Framework V4 Main tool for defining and deploying the entire AWS architecture. Compute AWS Lambda Handles business logic and runs the TypeScript code. API Gateway AWS API Gateway Synchronous HTTP communication gateway for the entire Backend. Database NeonDB (Serverless PostgreSQL) Primary database for relational data. ORM Prisma Abstraction Layer between the code and the database. Security AWS SSM Parameter Store Secure storage for sensitive environment variables. DevOps GitHub Actions Automates the CI/CD pipeline. 2. Required Resources \u0026amp; Software To complete the workshop, users must have the following tools and accounts ready on their computer.\nA. Account Requirements AWS Account: Necessary for deploying Serverless services (Lambda, API Gateway, SSM). NeonDB Account: Necessary for creating and retrieving the connection string (DATABASE_URL) for the PostgreSQL database. GitHub Account: Necessary for storing the source code and setting up CI/CD (GitHub Actions). B. Local Software \u0026amp; Tools Node.js (v20+): Installed and accessible via the terminal. npm or yarn: Package manager. AWS CLI: Necessary to configure AWS access permissions from the local machine for the Serverless Framework. IDE (VS Code): Recommended development environment. Postman/Insomnia: Essential tool for testing API Endpoints (GET/POST). C. Project Setup Serverless Framework CLI: Must be installed globally (npm install -g serverless). AWS Credentials: Configure AWS access permissions (User/Role) on the local machine. "
},
{
	"uri": "http://localhost:1313/repo-name/1-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\rAWS FIRST CLOUD AI JOURNEY – PROJECT PLAN TEEJ_SorcererXStreme - FPT University - Ho Chi Minh Campus - SORCERERXSTREME 0. TABLE OF CONTENTS CONTEXT AND MOTIVATION\n1.1. Summary 1.2. Project Success Criteria 1.3. Assumptions and Premises SOLUTION ARCHITECTURE\n2.1. Technical Architecture Diagram 2.2. Technical Plan 2.3. Project Plan 2.4. Security Factors ACTIVITIES AND DELIVERABLES\n3.1. Activities and Deliverables 3.2. Out of Scope 3.3. Operational Handover Process COST ANALYSIS BY SERVICE\nTEAM\nRESOURCES AND COST ESTIMATION\nACCEPTANCE\n1. CONTEXT AND MOTIVATION 1.1 Summary 1. Customer Context \u0026amp; Problem Problem: Current sources of spiritual information are fragmented, unverified, and lack personalization. Users struggle to find deep, reliable interpretations or compare Eastern vs. Western schools of knowledge (e.g., Eastern Horoscope vs. Astrology). Motivation: The urgent need of current customers is to build a unified platform capable of providing intellectually verified content while maintaining scalability and optimizing operational costs. 2. Business \u0026amp; Technical Goals Goal Type Details\rBusiness\r- Provide content with superior reliability and depth compared to current services. - Generate revenue from a paid service model.\rTechnical\r- Ensure AI Reliability: Apply RAG core to minimize AI \"hallucinations\". - Scalability: Build AWS Serverless architecture (Lambda, API Gateway) to easily handle high traffic and optimize operational costs (Pay-per-use).\r3. Use Cases Key functions the project will support for users:\nDeep Interactive AI Chat: Chat directly with AI, capable of maintaining context and combining multiple schools of thought in a single session. Personalized Interpretation: Provide in-depth reports based on input data (date of birth, place of birth, time of birth). Automated Notifications: Send periodic notifications via email. 4. Consulting Service Summary Professional services will be provided to achieve the above goals, including:\nServerless Architecture Design: Build a multi-model architecture and set up RAG flow on AWS Bedrock. Cost \u0026amp; Performance Optimization: Fine-tune Lambda functions and establish security via SSM Parameter Store. CI/CD Automation: Implement the entire development and deployment process automatically (IaC) using Serverless Framework and GitHub Actions. 1.2 Project Success Criteria The success of the project will be evaluated based on the following quantitative and qualitative criteria:\nData Reliability: The RAG system operates accurately, minimizing AI hallucinations, and providing transparent source citations for interpretations. Knowledge Unification: Successful integration of Eastern and Western metaphysical data into a single platform. Business Efficiency: Successful implementation of a tiered model (Free/VIP) to generate a stable revenue stream. Cost Optimization: System operates on Serverless architecture with an estimated cost of ~$9.06/month for the Demo environment. Scalability: System auto-scales to handle high traffic without manual intervention. 1.3 Assumptions and Premises The project is executed based on the following assumptions and constraints:\nTechnology Dependency: The project depends on the availability and stability of AWS services (Bedrock, Lambda, API Gateway). Data: It is assumed that input data for RAG (books, metaphysical documents) is clean, copyrighted, or within valid usage scope. Risks: There is a risk of LLM \u0026ldquo;hallucinations\u0026rdquo; despite using RAG; a Fact Checker mechanism is required. Cost Constraints: The operating budget is strictly optimized. 2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The proposed architecture is Hybrid Serverless on AWS, including layers: Edge \u0026amp; Auth, API \u0026amp; Routing, Compute, Data, AI/ML, and Async Monitoring.\nKey Components:\nFrontend: AWS Amplify (Next.js). Auth: Amazon Cognito. Backend: AWS Lambda, Amazon API Gateway. Database: NeonDB (PostgreSQL) as main DB, DynamoDB for history, Pinecone for Vector Search. AI Core: Amazon Bedrock (LLM \u0026amp; Embeddings), S3 (RAG Docs). 2.2 Technical Plan The project team will develop and deploy the system according to the following technical process:\nScripts \u0026amp; IaC: Use Serverless Framework to generate CloudFormation templates, ensuring infrastructure deployment (IaC) is repeatable and consistent. CI/CD: Use GitHub Actions to automate the Build, Test, and Deploy process for Lambda functions and API Gateway. RAG Pipeline: Set up data processing flow: Upload documents to S3 -\u0026gt; Lambda Trigger -\u0026gt; Generate Embedding (Bedrock) -\u0026gt; Store in Pinecone Vector DB. 2.3 Project Plan The project applies the Agile-Iterative model over 9 weeks, divided into 3 main phases (Iterations):\nIter 3 (Weeks 1-3): Redesign system, finalize SRS/SDS documentation, and build Prototype for RAG pipeline. Iter 4 (Weeks 4-6): Integrate AWS Cognito, develop permission logic (Guest/VIP), build data corpus on S3. Iter 5 (Weeks 7-9): Full deployment to AWS, End-to-End testing (QA), cost and performance optimization. 2.4 Security Factors Security is designed according to the \u0026ldquo;Defense in Depth\u0026rdquo; model:\nIdentity \u0026amp; Access: Use Amazon Cognito to manage identity and user permissions (User Roles). Data Protection: Secret keys (API Keys, DB Credentials) are stored securely in AWS Systems Manager Parameter Store, not hardcoded. Network Security: API Gateway acts as the sole entry point. Monitoring: Use CloudWatch to log and monitor abnormal behaviors. 3. ACTIVITIES AND DELIVERABLES 3.1 Activities and deliverables Deployment Phase Timeline Activities Milestones Completion Date Design \u0026amp; Prototype Weeks 1-3 - Design AWS architecture.\n- Collect RAG data.\n- Write SRS/SDS documentation. - Architecture Diagram \u0026amp; Cost Table.\n- RAG Pipeline Prototype.\n- Project Proposal Document. 12/10/-01/11/2025 Development (Core) Weeks 4-6 - Integrate Cognito.\n- Code Backend (Lambda).\n- Build Vector DB. - Auth System operational.\n- Complete API for VIP/Free.\n- RAG Knowledge Base ready. 02/11-22/11/2025 Deployment \u0026amp; QA Weeks 7-9 - Configure GitHub Actions.\n- Deploy to Prod environment.\n- Load Test \u0026amp; Pen Test. - System Live on AWS.\n- Test Report.\n- User Guide. 23/11-09/12/2025 3.2 Out of Scope The following items are outside the scope of the current project (MVP):\nMobile App development (iOS/Android). Real-time Voice Chat feature. 3.3 Operational Handover Process The current version is an MVP (Minimum Viable Product). To bring it to large-scale production, additional steps are required:\nFeature Expansion: Upgrade Lambda and Bedrock architecture to support React Native Mobile App or Voice Chat in the future. Operational Excellence: Set up more detailed AWS CloudWatch Alarms to monitor errors and latency. RAG Optimization: Fine-tune chunk sizes and retrieval strategies to reduce response latency. 4. COST ANALYSIS BY SERVICE Estimated cost for Demo environment (~5,000 requests/month) is $9.06/month.\nDetailed link: Cost Estimation Table\nLayer AWS Service Purpose Cost/Month Compute \u0026amp; API AWS Lambda, API Gateway, Amplify Backend Logic \u0026amp; Hosting ~$2.62 Data \u0026amp; Storage DynamoDB, S3 Chat History \u0026amp; RAG Storage ~$0.92 AI \u0026amp; Security Bedrock, Cognito, Parameter Store LLM Generation \u0026amp; Auth ~$2.65 Async \u0026amp; Monitoring SES, CloudWatch, EventBridge Email \u0026amp; Logging ~$2.88 Total $9.06 5. TEAM Overall Responsibility Name Title Description Email Nguyễn Gia Hưng Head of Architecture Solution Design and develop cloud-native and serverless platforms hunggia@amazon.com Stakeholders Name Title Description Email Đình Quang Sáng PQHDN Evaluation \u0026amp; Direction SangDQ6@fe.edu.vn Support Representatives Name Title Description Email Văn Hoàng Kha Cloud Security Engineer, Co-founded and led Viet-AWS Execute technical directions on Cloud Security and DevSecOps khavan.work@gmail.com Implementation Team Name Title Description Email Trần Phương Huyền Leader + Backend Dev Project Management + Backend Development tranphuonghuyen2005@gmail.com Nguyễn Lâm Anh AI Dev AI Development nguyenla110505@gmail.com Nguyễn Văn Linh AI Dev AI Development nguyenvanlinh.1710.it@gmail.com Bùi Nguyễn Tấn Khang Frontend Dev Frontend Development tankhang6a6@gmail.com 6. RESOURCES AND COST ESTIMATION Personnel Unit Price (Estimated personnel unit price based on student/academic project rates)\nResource / Role Responsibilities Unit Price (USD) / Hour Solution Architects - Overall architecture design (Hybrid Serverless, RAG Flow).\n- Selection of optimal AWS services (Bedrock, Lambda, Pinecone).\n- Ensuring non-functional requirements (security, latency, cost). 2.3 Software Engineers - Frontend development (Next.js/Amplify) and Cognito integration.\n- Backend API (Lambda) construction to handle user logic.\n- Chat History storage management (DynamoDB). 0.7 AI Engineers - Fine-tune Prompts for Bedrock for Tarot/Horoscope interpretation.\n- Build RAG flow: Document Chunking, Embeddings, Vector Search.\n- Evaluate accuracy and minimize AI hallucinations. 0.7 Estimated Hours \u0026amp; Cost by Phase (Effort estimation for 3 Iterations - 9 weeks)\nProject Phase Role Man-hours Cost (USD) Total Phase Cost Iter 3: Design \u0026amp; Prototype\n(Weeks 1-3) Solution Architect\nSoftware Engineer\nAI Engineer 30\n30\n30 $69.0\n$21.0\n$21.0 $111.0 Iter 4: Core Development\n(Weeks 4-6) Solution Architect\nSoftware Engineer\nAI Engineer 20\n80\n80 $46.0\n$56.0\n$56.0 $158.0 Iter 5: Deploy \u0026amp; QA\n(Weeks 7-9) Solution Architect\nSoftware Engineer\nAI Engineer 10\n60\n40 $23.0\n$42.0\n$28.0 $93.0 TOTAL 380 $362.0 Cost Contribution Allocation (Allocation of cost responsibility among stakeholders)\nParticipating Party Contribution Value (USD) Contribution Ratio of Total Partner (Team) $362.0 97.5% (Self-contributed personnel cost) AWS ~$9.06/month 2.5% (Estimated Cloud infrastructure cost) Customer $0 0% (Academic Project/MVP) 7. ACCEPTANCE The acceptance of the SorcererXStreme project will not only be based on feature completion but also on system stability and the quality of AI output.\n7.1. Deliverable Package The product is considered ready for acceptance when the project team provides all the following items:\nSource Code: Repository containing all Backend code (Lambda), Frontend code (Next.js), and Infrastructure as Code (Serverless Framework). Technical Documentation: Including SRS, SDS, API Documentation, and Deployment Guide. Live Environment: URL to access the stable functioning Demo environment on AWS. Quality Report: Test results (Test Cases) and accuracy assessment report of the RAG model. 7.2. Acceptance Process The process will take place in a 4-step sequence:\nLive Demo: The project team directly demos key User Flows: Registration -\u0026gt; Select Service -\u0026gt; Chat with AI -\u0026gt; Receive Results. User Acceptance Testing (UAT): The Customer/Mentor directly uses the system (03-05 days) to check the accuracy of spiritual knowledge and load capacity. Feedback \u0026amp; Fix: The project team commits to fixing Critical errors within 24-48 hours. Minor errors will be updated in subsequent patches. Completion Confirmation: The project is accepted when there are no critical errors and core features operate as committed. 7.3. Rejection Conditions The product will not be accepted if:\nDeployment Error: System Downtime or API error rate \u0026gt; 10%. Content Deviation: AI provides seriously misleading information or violates safety rules. Over Budget: Actual operating costs exceed the allowable threshold without reasonable explanation. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.1-preparation/",
	"title": "Setting up Development Environment",
	"tags": [],
	"description": "",
	"content": "1. Prerequisites To develop modern web applications, you need the following standard tools:\nNode.js (LTS Version): Runtime environment for JavaScript/TypeScript. Git: Distributed version control system. IDE: Visual Studio Code (recommended). Recommended VS Code Extensions:\nESLint \u0026amp; Prettier: Automate formatting and linting. Tailwind CSS IntelliSense: Rapid Tailwind class suggestions. ES7+ React/Redux/React-Native snippets: Code faster with shortcuts. 2. Initialize Next.js Project We will use Next.js - the most popular React Framework today.\nRun initialization command:\nnpx create-next-app@latest my-serverless-app Detailed Configuration:\nTypeScript: Yes (Type safety) Tailwind CSS: Yes (Rapid styling) ESLint: Yes (Linting) App Router: Yes (Latest routing architecture) Import Alias: @/ (Cleaner imports) 3. Initialize AWS Amplify (Backend) This is a crucial step to integrate Serverless features (Auth, Data) into your project. Run the following command inside your project folder:\ncd my-serverless-app npm create amplify@latest When prompt to install, select Yes. Amplify will automatically create the amplify/ folder containing the Backend structure.\n4. Install AWS Libraries Install necessary SDK packages for Frontend to communicate with AWS:\nnpm install aws-amplify @aws-amplify/ui-react 5. Standard Project Structure After installation, your folder structure should look like this:\namplify/: Contains Backend code (auth.ts, data.ts). src/app: Contains Pages and Layouts (App Router). src/components: Contains reusable UI Components. amplify_outputs.json: Auto-generated config file (Do not edit). 6. Configure tsconfig.json (Best Practices) To ensure strict TypeScript coding, update tsconfig.json:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es5\u0026#34;, \u0026#34;lib\u0026#34;: [\u0026#34;dom\u0026#34;, \u0026#34;dom.iterable\u0026#34;, \u0026#34;esnext\u0026#34;], \u0026#34;allowJs\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;strict\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true, \u0026#34;noEmit\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;module\u0026#34;: \u0026#34;esnext\u0026#34;, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;resolveJsonModule\u0026#34;: true, \u0026#34;isolatedModules\u0026#34;: true, \u0026#34;jsx\u0026#34;: \u0026#34;preserve\u0026#34;, \u0026#34;incremental\u0026#34;: true, \u0026#34;plugins\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;next\u0026#34; } ], \u0026#34;paths\u0026#34;: { \u0026#34;@/*\u0026#34;: [\u0026#34;./src/*\u0026#34;] } }, \u0026#34;include\u0026#34;: [\u0026#34;next-env.d.ts\u0026#34;, \u0026#34;**/*.ts\u0026#34;, \u0026#34;**/*.tsx\u0026#34;, \u0026#34;.next/types/**/*.ts\u0026#34;], \u0026#34;exclude\u0026#34;: [\u0026#34;node_modules\u0026#34;] } 7. Install UI Libraries To create the \u0026ldquo;Mystical\u0026rdquo; UI:\nnpm install framer-motion lucide-react clsx tailwind-merge 8. Configure TailwindCSS Set up colors in tailwind.config.ts:\n// tailwind.config.ts import type { Config } from \u0026#34;tailwindcss\u0026#34;; const config: Config = { content: [ \u0026#34;./src/pages/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/components/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/app/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, ], theme: { extend: { colors: { primary: \u0026#34;#432c7a\u0026#34;, secondary: \u0026#34;#764ba2\u0026#34;, accent: \u0026#34;#ffd700\u0026#34;, background: \u0026#34;#1a0b2e\u0026#34;, }, backgroundImage: { \u0026#34;gradient-radial\u0026#34;: \u0026#34;radial-gradient(var(--tw-gradient-stops))\u0026#34;, }, }, }, plugins: [], }; export default config; My Experience Amplify Gen 2 vs Gen 1: If you used Amplify CLI (Gen 1) before with commands like amplify add auth, forget it! Gen 2 (what we are using) is Code-First. You define Backend using TypeScript (in amplify/ folder) instead of clicking through Console. It gives Frontend Devs much better control over infrastructure.\nVerification \u0026amp; Testing Test Case: Check Amplify Installation\nOpen package.json. Look inside dependencies. Expected Result: You should see \u0026quot;aws-amplify\u0026quot;: \u0026quot;^6.x.x\u0026quot; and \u0026quot;@aws-amplify/backend\u0026quot;: \u0026quot;^1.x.x\u0026quot;. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme: Building an AI-Powered Interpretation Platform on AWS Overview SorcererXtreme AI is a pioneering metaphysical guidance platform that leverages AI and AWS Serverless architecture to provide personalized, grounded, and reliable readings in Astrology, Tarot, Horoscopes, and Numerology.\n1. Frontend Development This section focuses on building the React/Next.js user interface and integrating with AWS Amplify.\nObjective Technology \u0026amp; Concepts Output Product Interface \u0026amp; UX Build core components (Chat UI, Profile Settings, Payment Gateways). Intuitive, responsive user interface for services: Tarot Reading, Astrology Chart. Authentication Integrate AWS Cognito and Amplify Authenticator into the Frontend. Fully functional sign-up/sign-in system. Backend Integration Write Client-side API calls (axios) functions to invoke the API Gateway Endpoint (Backend). Working Fetch data/Post requests functions, displaying data (Response) from Lambda. 2. Backend Development This is the core of the Serverless architecture, focusing on business logic and performance optimization.\nObjective Technology \u0026amp; Concepts Output Product API \u0026amp; DB Layer Set up Express.js and serverless-http on AWS Lambda. Configure Prisma and secure connection to NeonDB. Basic Lambda functions working: UserAPI (Profile CRUD) and ReminderService. Asynchronous Architecture Build the Reminder Service flow using EventBridge Scheduler and Amazon SES. The findUsersToRemind logic is operational and automatically sends notification emails. Optimization Optimize the Serverless/Prisma deployment package, and set up minimum IAM Roles. Backend source code is automatically deployed via GitHub Actions (CI/CD). 4. AI Development The most crucial part, where the RAG logic is established to deliver intelligent content.\nObjective Technology \u0026amp; Concepts Output Product RAG Core Understand and set up the Retrieval-Augmented Generation (RAG) flow. Operational RAG architecture. Bedrock \u0026amp; Embeddings Use Amazon Bedrock to create Vector Embeddings from user questions and call LLMs. The Lambda function (ChatbotAPI) successfully calls Bedrock to generate answers. Data Retrieval Use Pinecone as a Vector Database to store and retrieve knowledge Chunks from the RAG knowledge base (stored in S3). The process of context retrieval and passing that context into the Prompt to generate accurate answers. Content Workshop overview Frontend Development Backend Development AI Development "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.2-ui-implementation/",
	"title": "Building Modern User Interface",
	"tags": [],
	"description": "",
	"content": "1. Layout Design (Responsive \u0026amp; Glassmorphism) Our goal is to create a mystical, \u0026ldquo;AI Sorcerer\u0026rdquo; interface like the Mockup below:\nUse CSS Grid and Flexbox from Tailwind to create flexible layouts.\n// src/app/layout.tsx export default function RootLayout({ children }: { children: React.ReactNode }) { return ( \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;body className=\u0026#34;bg-background text-white min-h-screen bg-[url(\u0026#39;/bg-stars.png\u0026#39;)] bg-cover\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-black/50\u0026#34; /\u0026gt; {/* Overlay */} \u0026lt;main className=\u0026#34;relative z-10 container mx-auto px-4 py-8 grid grid-cols-1 lg:grid-cols-12 gap-6\u0026#34;\u0026gt; {/* Sidebar takes 3 cols on Desktop */} \u0026lt;aside className=\u0026#34;lg:col-span-3 hidden lg:block\u0026#34;\u0026gt; {/* Sidebar Content */} \u0026lt;/aside\u0026gt; {/* Main Content takes 9 cols */} \u0026lt;section className=\u0026#34;lg:col-span-9\u0026#34;\u0026gt; {children} \u0026lt;/section\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ) } 2. Component: Horoscope Widget Create a Widget displaying daily horoscope with Glassmorphism effect.\n// src/components/HoroscopeWidget.tsx import { Star } from \u0026#39;lucide-react\u0026#39;; export default function HoroscopeWidget({ sign, prediction }: { sign: string, prediction: string }) { return ( \u0026lt;div className=\u0026#34;p-6 rounded-2xl bg-white/10 backdrop-blur-lg border border-white/20 hover:bg-white/20 transition-all duration-300 group\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3 mb-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;p-3 rounded-full bg-accent/20 text-accent group-hover:scale-110 transition-transform\u0026#34;\u0026gt; \u0026lt;Star size={24} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3 className=\u0026#34;text-xl font-bold\u0026#34;\u0026gt;{sign}\u0026lt;/h3\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p className=\u0026#34;text-gray-300 leading-relaxed\u0026#34;\u0026gt;{prediction}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); } 3. Component: Chat Interface Build chat interface with auto-scroll and dynamic message styling.\n// src/components/ChatBox.tsx import { useEffect, useRef } from \u0026#39;react\u0026#39;; import { clsx } from \u0026#39;clsx\u0026#39;; export default function ChatBox({ messages }: { messages: Message[] }) { const bottomRef = useRef\u0026lt;HTMLDivElement\u0026gt;(null); // Auto-scroll to bottom useEffect(() =\u0026gt; { bottomRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }, [messages]); return ( \u0026lt;div className=\u0026#34;flex flex-col h-[600px] bg-white/5 rounded-xl border border-white/10 overflow-hidden\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex-1 overflow-y-auto p-4 space-y-4\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;div key={idx} className={clsx( \u0026#34;max-w-[80%] p-3 rounded-lg\u0026#34;, msg.role === \u0026#39;user\u0026#39; ? \u0026#34;bg-primary self-end ml-auto\u0026#34; : \u0026#34;bg-white/10 self-start mr-auto\u0026#34; )}\u0026gt; {msg.content} \u0026lt;/div\u0026gt; ))} \u0026lt;div ref={bottomRef} /\u0026gt; \u0026lt;/div\u0026gt; {/* Input Area */} \u0026lt;/div\u0026gt; ); } 4. Animation with Framer Motion Add smooth entrance effects for UI elements.\nimport { motion } from \u0026#39;framer-motion\u0026#39;; const fadeIn = { hidden: { opacity: 0, y: 20 }, visible: { opacity: 1, y: 0 } }; \u0026lt;motion.div initial=\u0026#34;hidden\u0026#34; animate=\u0026#34;visible\u0026#34; variants={fadeIn} transition={{ duration: 0.5 }} \u0026gt; \u0026lt;HoroscopeWidget sign=\u0026#34;Leo\u0026#34; prediction=\u0026#34;Today is your lucky day!\u0026#34; /\u0026gt; \u0026lt;/motion.div\u0026gt; My Experience Mobile-First or Desktop-First? Initially, I designed for Desktop first, and when I opened it on mobile, the layout was broken. Lesson: Always use classes like hidden lg:block or grid-cols-1 lg:grid-cols-12 to prioritize Mobile layout first (default), then override for larger screens. TailwindCSS makes this incredibly easy!\nVerification \u0026amp; Testing Test Case 1: Responsive Design\nOpen browser on Desktop: See Sidebar on left, Chatbox on right. Press F12, switch to Mobile mode (iPhone 12/14). Expected Result: Sidebar hidden, Chatbox takes full width. Test Case 2: Hover Effects\nHover over HoroscopeWidget. Expected Result: Background brightens (bg-white/20), star icon scales up slightly (scale-110). "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.2-data-strategy/",
	"title": "Data Strategy",
	"tags": [],
	"description": "",
	"content": "This section required the most intensive research effort to ensure the quality of AI responses (Garbage In, Garbage Out).\n1. Format Standardization Journey: From JSON to JSONL The choice of file storage format plays a decisive role in processing performance and AWS Lambda memory costs. After extensive testing, the system standardized the entire Knowledge Base (Tarot, Horoscope, Numerology) to the JSONL (.jsonl) format. In this format, each line of the file is a distinct, valid JSON object, independent of the others. Why JSONL? Memory Optimization: Utilizes Lazy Loading mechanisms instead of loading the entire file. This keeps Lambda RAM consumption low and stable, completely eliminating Out of Memory (OOM) errors with large datasets. Local Fault Tolerance: A syntax error in one line does not break the entire pipeline. The system automatically skips the erroneous line and continues processing, ensuring data flow continuity. Streaming Compatibility: Supports direct stream reading from S3, minimizing latency when initiating Batch processing. Comparison with Legacy Solution (Standard JSON) The initial standard JSON format revealed critical performance weaknesses:\nFeature Legacy Solution: Standard JSON (.json) Current Solution: JSONL (.jsonl) Structure - Monolithic Array. - Enclosed by [...], separated by commas. - Independent Lines. - Each line is a separate object. Performance - Memory Hog: Must load the entire file into RAM to parse the DOM. - Memory Safe: Processing consumes RAM per line only. Risk - Single Point of Failure: One missing comma = Entire file corrupted. - Isolated: Error in line 1 does not affect line 2. Structure Illustration:\nJSON (Legacy - Array): [ {\u0026#34;id\u0026#34;: 1, \u0026#34;text\u0026#34;: \u0026#34;Aries...\u0026#34;}, {\u0026#34;id\u0026#34;: 2, \u0026#34;text\u0026#34;: \u0026#34;Taurus...\u0026#34;} ] Figure 2.1: Data sample in standard JSON format.\nJSONL (New - Line-delimited): {\u0026#34;id\u0026#34;: 1, \u0026#34;text\u0026#34;: \u0026#34;Aries...\u0026#34;} {\u0026#34;id\u0026#34;: 2, \u0026#34;text\u0026#34;: \u0026#34;Taurus...\u0026#34;} Figure 2.2: Data configuration in JSONL format.\n2. \u0026ldquo;Divide and Conquer\u0026rdquo; Technique in Embedding Challenge: Raw text data is often too long and contains significant keyword noise. When RAG (Retrieval-Augmented Generation) queries entire large paragraphs, the AI\u0026rsquo;s focus is easily \u0026ldquo;diluted,\u0026rdquo; leading to rambling responses. Solution: Instead of embedding the entire text, the system implements: Flatten Contexts: Breaks down information fields within contexts (e.g., strengths, weaknesses, love). Meta-Injection: Attaches metadata (Name, Category, Keyword) to each micro-chunk before embedding. Result: During vector search, the system extracts the precise segment required (e.g., retrieving only the \u0026ldquo;Love aspects of Aries\u0026rdquo; segment rather than the entire Aries article), enabling the LLM to provide focused, accurate answers. 3. Database Design (DynamoDB) To ensure sub-10ms latency for Real-time applications, the system utilizes Amazon DynamoDB with a dual-table design, serving two core purposes: Knowledge Storage and Conversation Context Storage.\nFigure 3.1: Overview of active DynamoDB tables.\nMetaphysicalKnowledgeBase Table This acts as the \u0026ldquo;Source of Truth\u0026rdquo; for the system, storing detailed information on Tarot, Zodiac signs, and Numerology. Data here serves as the Ground Truth for the RAG process.\nPartition Key (PK): category (String). E.g., Tarot_card, Zodiac_sign. Enables efficient querying by grouping large datasets by type. Sort Key (SK): entity_name (String). E.g., Ace of Cups, Aries. Unique identifier for each entity within a category. Item Structure Detail: Instead of storing flat text, the system stores structured JSON within the contexts attribute. This allows flexible extraction of specific aspects (Love, Career, Health) rather than retrieving the entire document.\nFigure 3.2: Attribute details for a Tarot card (Ace of Cups).\nAttribute Name Type Description contexts String (JSON) Contains categorized content segments (general_upright, love_reversed, etc.). This is the primary source for Embedding. keywords List List of related keywords to support hybrid search alongside Semantic Search. Chat History Table (sorcererxtreme-chatMessages) To enable the AI to \u0026ldquo;remember\u0026rdquo; previous interactions (Context Retention), a short-term memory system is required. This table stores the entire conversation history per session.\nPartition Key (PK): sessionId (String). Unique identifier for the user\u0026rsquo;s chat session. Sort Key (SK): timestamp (String). Message timestamp (ISO 8601), ensuring conversation logs are ordered chronologically. Figure 3.3: Data sample of a stored user message.\nProcessing Flow: Whenever a user sends a new message:\nThe system queries this table using the current sessionId. Retrieves the last $N$ messages (Context Window). Injects this history into the LLM Prompt to ensure coherent, context-aware responses. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/",
	"title": "Frontend Development",
	"tags": [],
	"description": "",
	"content": "Building Serverless Frontend with Next.js \u0026amp; AWS Amplify 1. Workshop Overview Welcome to the Building Modern Serverless Frontend workshop for the SorcererXtreme project.\nIn this project, the Frontend acts as the \u0026ldquo;face\u0026rdquo; of the application, where users directly interact with the mystical features. Our mission is to build a beautiful, smooth interface that communicates effectively with the backend AWS services.\n2. Frontend Architecture We will focus on the Client (Frontend) architecture and its Integration Points:\nFrontend Workflow: Hosting \u0026amp; Delivery: Next.js code is hosted and operated on AWS Amplify. Users access the web app via a global CDN (CloudFront) built into Amplify, ensuring lightning-fast load times. Authentication: When a user Logs In, the Frontend communicates directly with Amazon Cognito. Cognito returns a \u0026ldquo;Token\u0026rdquo; (like an access pass). API Interaction: For every request (like chatbot or Tarot reading), the Frontend sends this Token along with the request to Amazon API Gateway. Response: The Frontend receives JSON results from the API and renders the UI. The Frontend does not need to know what Database or AI model is behind the API; it only cares about the Input (Request) and Output (Response). 3. Frontend Tech Stack The \u0026ldquo;weaponry\u0026rdquo; of a Frontend Developer in this project:\nTechnology Role Why use it? Next.js (App Router) Framework Strong Server-Side Rendering (SSR) for SEO, powerful Router. AWS Amplify (Gen 2) Platform Provides Hosting, automated CI/CD, and fast Cloud connection libraries. Tailwind CSS Styling Rapid styling, easy customization for the mystical \u0026ldquo;Dark Mode\u0026rdquo;. Framer Motion Animation Create smooth motion effects (like 3D Tarot card flipping). Amplify UI Library Pre-built components for Login/Registration flows. Axios / Fetch HTTP Client Used to call API Gateway. 4. Estimated Time \u0026amp; Cost Item Details Time 2-3 hours per day Cost ~$9.06/month (Total project) 5. Workshop Content We will follow a standard Frontend development process:\nPreparation: Setup Next.js and Amplify. UI Implementation: Code Chat \u0026amp; Tarot UI with animations. Integration: Integrate Login (Cognito) and API calls (Gateway). CI/CD Pipeline: Push code to Git and auto-deploy to the Internet. Advanced: Custom Domain setup and SEO optimization. Backend Reference: Understanding RAG model. Cleanup: Cleaning up resources. Frontend Mindset: In Serverless architecture, Frontend is not just a \u0026ldquo;renderer\u0026rdquo;. It is also responsible for Security (keeping Tokens safe) and User Experience (handling Loading states while waiting for AI). Pay attention to these points during the workshop!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.2-set-up/",
	"title": "Setup environment",
	"tags": [],
	"description": "",
	"content": "Step 1: Project Initialization \u0026amp; Library Installation Create the project directory and install all the necessary core dependencies.\n# Create the main directory for the Backend mkdir my-serverless-backend \u0026amp;\u0026amp; cd my-serverless-backend # Initialize a Node.js package npm init -y # Install Core Dependencies: Express, Prisma Client, Serverless-HTTP, etc. npm install express cors dotenv @prisma/client axios serverless-http # Install Development Dependencies: TypeScript, Types for Node/Express, Prisma CLI, Serverless-Offline npm install -D typescript @types/node @types/express serverless-offline prisma serverless-dotenv-plugin Step 2: TypeScript \u0026amp; Directory Structure Setup Configure TypeScript and create the standard directory structure.\nInitialize tsconfig.json: npx tsc --init Edit tsconfig.json: Open the tsconfig.json file and adjust the following settings to ensure modern Node.js source code and compatibility with Lambda: \u0026quot;target\u0026quot;: \u0026quot;ES2020\u0026quot; (or newer) \u0026quot;module\u0026quot;: \u0026quot;commonjs\u0026quot; \u0026quot;outDir\u0026quot;: \u0026quot;./dist\u0026quot; (Output directory for compiled code) \u0026quot;rootDir\u0026quot;: \u0026quot;./src\u0026quot; (Source code directory) \u0026quot;esModuleInterop\u0026quot;: true \u0026quot;strict\u0026quot;: true Create Directory Structure: mkdir src src/routes src/services src/controllers Step 3: Code Restructuring (Express -\u0026gt; Lambda) Since Lambda does not \u0026ldquo;listen\u0026rdquo; on a standard port, we use serverless-http to wrap Express.\nFile src/app.ts (Core Express App):\nimport express from \u0026#39;express\u0026#39;; import cors from \u0026#39;cors\u0026#39;; import routes from \u0026#39;./routes/index\u0026#39;; // Change to index if you use routes/index.ts const app = express(); // 1. Basic Middlewares app.use(cors({ origin: process.env.FRONTEND_URL || \u0026#39;*\u0026#39; })); app.use(express.json()); // 2. API Routing (Main Endpoint will be /api/...) app.use(\u0026#39;/api\u0026#39;, routes); // IMPORTANT: Do not use app.listen(), eliminate traditional web server logic. export default app; File src/handler.ts (Lambda Bridge):\nimport serverless from \u0026#34;serverless-http\u0026#34;; import app from \u0026#34;./app\u0026#34;; // Export the main handler that AWS Lambda will call export const handler = serverless(app); Step 4: Configure Prisma \u0026amp; NeonDB Connection To connect to NeonDB and ensure the Prisma Client works in the AWS Lambda Linux environment, the binaryTargets must be configured.\nCreate Schema File \u0026amp; Configure binaryTargets: npx prisma init Open the file prisma/schema.prisma and add the configuration: generator client {\rprovider = \u0026#34;prisma-client-js\u0026#34;\r// native: For the dev machine (Mac/Win)\r// rhel-openssl-3.0.x: For AWS Lambda (Node 20)\rbinaryTargets = [\u0026#34;native\u0026#34;, \u0026#34;rhel-openssl-3.0.x\u0026#34;] }\rdatasource db {\rprovider = \u0026#34;postgresql\u0026#34;\rurl = env(\u0026#34;DATABASE_URL\u0026#34;)\r}\r// ... model definitions (User, Partner, Reminder, etc.) Create Local .env file: Create the .env file and paste your NeonDB connection string into it. # Get the PostgreSQL connection string from the Neon Console DATABASE_URL=\u0026#34;postgresql://[user]:[password]@[endpoint]/[dbname]?sslmode=require\u0026#34; Generate Prisma Client: Run the following command to create the Prisma Client and download the necessary binaries. npx prisma generate Complete: Your local environment is now ready. You can compile the code (npm run build) and run local tests with serverless-offline.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/",
	"title": "Backend Development",
	"tags": [],
	"description": "",
	"content": "Building and Automating with Serverless Backend (Serverless V4) This guide is a comprehensive document detailing the entire Backend development process for the SorcererXtreme AI project, from setting up the local development environment to completing the automated deployment workflow (CI/CD) on AWS.\n1. Context \u0026amp; Technical Challenges In the development of high-performance Serverless applications, we face several complex technical hurdles:\nFramework Transition: The transition from a traditional HTTP Framework like Express.js to the stateless environment of AWS Lambda requires using serverless-http and a change in source code structure. Database Management: Using a sophisticated ORM like Prisma on a Serverless platform necessitates techniques to optimize the deployment package size to under 250MB and requires downloading the correct Prisma Binary (rhel-openssl-3.0.x) for the Lambda Linux environment. Security: Ensuring that sensitive connection strings are never stored in the source code, but are securely managed using AWS SSM Parameter Store. 2. Core Value of the Constructed Architecture Our Backend architecture is designed to overcome these challenges, delivering key benefits:\nCost and Performance Optimization: Utilizing AWS Lambda and Serverless Framework V4 ensures you only pay for the time your code actually runs. Optimizing the Prisma package significantly reduces Cold Start time. Full Automation (CI/CD): Building the GitHub Actions workflow automates the entire development loop: Code -\u0026gt; Push -\u0026gt; Build -\u0026gt; Deploy, completely eliminating manual deployment steps and minimizing human error. Data Flexibility: Establishing a stable and secure connection to an external Database (NeonDB) demonstrates the capability for flexible integration in real-world projects. This guide will serve as a detailed, step-by-step map to help you master the entire process of modern Serverless Backend development.\nPrepare Set up environment Configure Serverless Framework Save secret key Automation CI/CD Connect microservices Set up email Development workflow "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.3-configure-serverless/",
	"title": "Configure Serverless Framework",
	"tags": [],
	"description": "",
	"content": "This is the most crucial step, where we define the AWS infrastructure (IaC) and optimize the deployment package for Lambda.\n1. The serverless.yml File This configuration file includes sections for SSM Security, Build/Package Optimization, and necessary IAM Roles.\n# serverless.yml org: your_organization_name service: my-serverless-backend # Ensure service name matches the directory provider: name: aws runtime: nodejs20.x region: ap-southeast-1 timeout: 29 # Maximum allowed by API Gateway (keep as is) memorySize: 512 # Lambda memory configuration (recommended for Prisma) # --- VPC Network Configuration (MANDATORY for NeonDB) --- # If NeonDB requires a fixed IP or you use an internal RDS, # Lambda needs VPC configuration to connect externally or within the VPC. # (Assumed for NeonDB Public Access, but VPC needed if using Private Subnet) # vpc: # securityGroupIds: [sg-xxxxxxxx] # subnetIds: [subnet-xxxxxx] # --- Environment Variables (Fetched from SSM) --- environment: # Retrieve secrets from AWS SSM Parameter Store (Encrypted) DATABASE_URL: ${ssm:/my-app/${self:provider.stage}/database_url} JWT_SECRET: ${ssm:/my-app/${self:provider.stage}/jwt_secret} FRONTEND_URL: ${ssm:/my-app/${self:provider.stage}/frontend_url, \u0026#39;http://localhost:3000\u0026#39;} # Add local fallback PRISMA_CLI_BINARY_TARGETS: rhel-openssl-3.0.x # Crucial for Prisma # --- IAM Permissions (Required) --- # Add necessary permissions for Lambda to read SSM and access other services iam: role: statements: - Effect: \u0026#39;Allow\u0026#39; Action: - \u0026#39;ssm:GetParameter\u0026#39; Resource: \u0026#39;arn:aws:ssm:${self:provider.region}:*:parameter/my-app/${self:provider.stage}/*\u0026#39; # --- DDoS/Billing Protection (Keep as is) --- apiGateway: usagePlan: quota: limit: 5000000 period: MONTH throttle: burstLimit: 200 rateLimit: 100 # --- Deployment Package Size Optimization --- build: esbuild: bundle: true minify: true sourcemap: false # Explicitly specify modules to exclude from the main package external: - \u0026#39;aws-sdk\u0026#39; - \u0026#39;@prisma/client/runtime/library\u0026#39; package: individually: true patterns: - \u0026#39;src/handler.js\u0026#39; - \u0026#39;src/app.js\u0026#39; - \u0026#39;src/**/*.js\u0026#39; - \u0026#39;dist/**/*.js\u0026#39; # Ensure compiled JS files are packaged - \u0026#39;package.json\u0026#39; - \u0026#39;node_modules/**\u0026#39; # --- Define Prisma Binary Files (Extremely important) --- # Only include the necessary Linux binary file to keep package \u0026lt; 250MB - \u0026#39;node_modules/.prisma/client/libquery_engine-rhel-openssl-3.0.x.so.node\u0026#39; - \u0026#39;node_modules/.prisma/client/schema.prisma\u0026#39; - \u0026#39;!./**\u0026#39; # Remove all unnecessary files after defining required patterns above - \u0026#39;!node_modules/aws-sdk/**\u0026#39; # Reduce size by excluding SDK already available in Lambda plugins: - serverless-offline - serverless-dotenv-plugin functions: api: handler: src/handler.handler events: - http: { path: /, method: ANY } - http: { path: /{proxy+}, method: ANY } 2. Required Steps A. Store Secrets in AWS SSM (Security) Before deployment, you must store sensitive values in the AWS SSM Parameter Store so the Serverless Framework can read them.\n# Command to store DATABASE_URL (SecureString) aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:password@endpoint...\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # Repeat for JWT_SECRET and FRONTEND_URL B. Run Local Testing Use the serverless-offline plugin to run the API locally, connecting directly to NeonDB via the .env file.\n# Run local API on port 3000 (default) sls offline start C. First-time Deployment Once the code has been locally tested, you are ready to deploy the entire infrastructure to AWS.\n# Deploy all resources (Lambda, API Gateway, IAM) sls deploy "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.3-integration/",
	"title": "Integrating API Gateway &amp; Authentication",
	"tags": [],
	"description": "",
	"content": "1. Authentication Flow Before coding, let\u0026rsquo;s understand how Frontend communicates with Cognito and API Gateway:\nUser enters User/Pass. Cognito returns JWT Token (ID Token, Access Token). Frontend sends Request with Token in Authorization Header. API Gateway verifies Token. If valid -\u0026gt; Forward to Lambda. 2. Configure AWS Amplify Install aws-amplify library to connect Frontend with AWS services:\nnpm install aws-amplify @aws-amplify/ui-react Configure in src/app/layout.tsx:\n\u0026#39;use client\u0026#39;; import { Amplify } from \u0026#39;aws-amplify\u0026#39;; import config from \u0026#39;@/amplifyconfiguration.json\u0026#39;; Amplify.configure(config); 3. Integrate Amazon Cognito (Authentication) Use Authenticator component to create a secure login/signup flow:\nimport { Authenticator } from \u0026#39;@aws-amplify/ui-react\u0026#39;; export default function LoginPage() { return ( \u0026lt;Authenticator\u0026gt; {({ signOut, user }) =\u0026gt; ( \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;Welcome, {user?.username}\u0026lt;/h1\u0026gt; \u0026lt;button onClick={signOut}\u0026gt;Sign Out\u0026lt;/button\u0026gt; \u0026lt;/main\u0026gt; )} \u0026lt;/Authenticator\u0026gt; ); } 4. Custom Hook: useAuth (Best Practices) Instead of calling fetchAuthSession everywhere, create a Custom Hook to reuse authentication logic and token retrieval.\n// src/hooks/useAuth.ts import { fetchAuthSession } from \u0026#39;aws-amplify/auth\u0026#39;; import { useState, useEffect } from \u0026#39;react\u0026#39;; export function useAuth() { const [token, setToken] = useState\u0026lt;string | null\u0026gt;(null); useEffect(() =\u0026gt; { const getToken = async () =\u0026gt; { try { const session = await fetchAuthSession(); setToken(session.tokens?.idToken?.toString() || null); } catch (err) { console.error(\u0026#34;Error fetching auth session\u0026#34;, err); } }; getToken(); }, []); return { token }; } 5. API Call with Error Handling Handle errors professionally using try/catch/finally and display feedback to users.\n// src/services/api.ts export const chatWithAI = async (message: string, token: string) =\u0026gt; { try { const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/chat`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: `Bearer ${token}` }, body: JSON.stringify({ message }) }); if (!response.ok) { if (response.status === 401) throw new Error(\u0026#34;Session expired\u0026#34;); if (response.status === 429) throw new Error(\u0026#34;Too many requests\u0026#34;); throw new Error(\u0026#34;System error\u0026#34;); } return await response.json(); } catch (error) { console.error(\u0026#34;API Error:\u0026#34;, error); throw error; // Throw error for UI to handle } }; My Experience Never expose API Keys! Once I accidentally committed a .env file containing API Keys to GitHub. AWS sent a warning email immediately. Solution: Always add .env to .gitignore. With Amplify, amplifyconfiguration.json is safe to be public as it only contains Resource IDs (like User Pool ID), not Secret Keys.\nVerification \u0026amp; Testing Test Case 1: Successful Login\nGo to Login page, enter created User/Pass. Click Sign In. Expected Result: Redirect to main page, display \u0026ldquo;Welcome, [Username]\u0026rdquo;. Test Case 2: Token Check\nOpen DevTools (F12) -\u0026gt; Network Tab. Send a Chat message. Find request sent to API Gateway. Check Header: Must have Authorization: Bearer eyJra... line (JWT Token). "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.3-storage-retrieval/",
	"title": "Storage &amp; Retrieval",
	"tags": [],
	"description": "",
	"content": "The system utilizes a Hybrid Retrieval mechanism (combining Vector Search and Key-Value Lookup). The core of this architecture is the use of Pinecone as the Long-term Memory for the AI.\n1. Vector Database: The Power of Pinecone Instead of self-managing infrastructure, the project selected Pinecone—a specialized Managed Vector Database. This component is crucial for enabling the AI to \u0026ldquo;understand\u0026rdquo; the semantics of Tarot or Horoscope inquiries, rather than relying solely on keyword matching.\nIndex Configuration Based on the Serverless architecture, the system utilizes Pinecone\u0026rsquo;s Serverless Index mode to optimize costs (paying only for data read/write, with no server maintenance fees).\nFigure 3.1: Actual Index Configuration on Pinecone Console.\nKey Technical Specifications:\nDimensions: 1024. This vector length is sufficient to encode the complex semantic nuances of metaphysical texts while remaining more optimal than 4096-dimensional models (too heavy) or 768-dimensional models (potentially lacking detail). Metric: Cosine Similarity. The system uses Cosine to measure the angle between two vectors. In semantic space, the smaller the angle between two vectors (Cosine approaching 1), the more similar their meanings. This metric is best suited for NLP (Natural Language Processing) tasks compared to Euclidean distance. Pod Type: Serverless (Automatically scales based on demand, no pre-provisioning required). Record Structure The true power of Pinecone lies in its ability to combine Vector Search with Metadata Filtering.\nFigure 3.2: Detail of a Vector Record including ID, Values, and Metadata.\nEach stored Record consists of three parts:\nID: Unique identifier (Hashed from original content) to prevent data duplication (Dedup). Values (Vector): An array of 1024 floating-point numbers, representing the meaning of the text segment. Metadata: This is the most critical part for increasing Accuracy. Instead of searching the \u0026ldquo;entire ocean,\u0026rdquo; the AI uses metadata to narrow the scope. Example: When a user asks about \u0026ldquo;Leo\u0026rsquo;s Love Life,\u0026rdquo; the system filters by category: \u0026quot;zodiac\u0026quot; and entity_name: \u0026quot;Leo\u0026quot; first, before finding the nearest vector. This completely eliminates the possibility of the AI retrieving information for the wrong sign. 2. Trade-off Analysis: Why Pinecone over AWS RDS (pgvector)? In the AWS ecosystem, the standard solution is often Amazon RDS for PostgreSQL with the pgvector extension. However, after rigorous trade-off consideration, Pinecone was selected.\nBelow is the detailed comparison analysis:\nCriteria Pinecone (Managed SaaS) Amazon RDS + pgvector (Self-Managed) Architecture Native Vector DB. Designed from the core specifically for high-speed vector storage and retrieval. Relational DB + Extension. A traditional relational database with vector processing capabilities \u0026ldquo;bolted on.\u0026rdquo; Operations (Ops) Zero Ops. No server management, no manual index tuning. Pure API calls. High Ops. Requires instance management, version updates, manual index configuration (IVFFlat/HNSW), and periodic database vacuuming. Latency Ultra-low (\u0026lt;50ms). Optimized for large-scale vector queries. Dependent on hardware configuration (CPU/RAM) and index tuning. Prone to slowness with large data if not well-optimized. Cost Pay-as-you-go. Billed based on stored vectors and R/W operations. Very cost-effective for project initiation. Fixed Hourly Cost. Must pay for instances running 24/7 even with zero usage (unless using Aurora Serverless v2, which is costly). Filtering Native Pre-filtering. Supports filtering metadata before vector search (Single-stage filtering), which is very powerful. Requires combining SQL WHERE clauses with vector search, which can sometimes impact performance if indexing is not precise. Conclusion: Given the current project scale and the requirement for rapid deployment (Time-to-market), Pinecone is the optimal choice due to its Serverless nature, allowing the team to focus on AI feature development rather than Database Administration (DBA) tasks.\n3. Why is DynamoDB still needed? Although Pinecone is powerful for Semantic Search, the system still requires DynamoDB for:\nExact Match Retrieval: Lambda Metaphysical needs to retrieve absolutely precise information based on IDs (e.g., the meaning of \u0026ldquo;The Fool\u0026rdquo; card when drawn, or specific star information in \u0026ldquo;Tu Vi\u0026rdquo;). DynamoDB performs this faster and cheaper than vector search. Latency Reduction: Offloads non-inferential tasks from the Vector DB. Raw Dataset Storage: Serves as a backup source and allows for quick metadata lookup without vector decoding. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/",
	"title": "AI Development",
	"tags": [],
	"description": "",
	"content": "1. Introduction This document serves as a Technical Case Study detailing the process of building a multi-domain AI consulting system (Tarot, Numerology, Horoscopes, Astrology) based on Serverless architecture. The core objective is to solve the AI \u0026ldquo;hallucination\u0026rdquo; problem through a rigorous RAG (Retrieval-Augmented Generation) pipeline, ensuring responses strictly adhere to the verified Knowledge Base.\nProblem Statement: AI Challenges in Esoteric Domains Developing AI for the esoteric domain presents unique technical barriers that standard chatbot solutions often fail to address:\nAbstract Nature and Ambiguity: The data contains numerous Sino-Vietnamese terms and abstract concepts (e.g., card meanings changing based on spread positions, planetary influences shifting by birth hour). General-purpose LLMs often struggle to grasp this narrow context. Requirement for Absolute Accuracy (Zero Hallucination): Users seeking advice demand precision. AI fabricating knowledge (such as incorrect star positions or wrong number interpretations) is a critical risk that must be eliminated. Unstructured Data Processing: The input Knowledge Base is heterogeneous, requiring a complex ETL process to standardize data into Vector Embeddings without losing original semantic meaning. Cost Optimization Strategy A top priority for this project is optimizing operating expenses (OpEx) from Day 1. Instead of maintaining expensive server clusters 24/7, the system fully adopts a Pay-as-you-go model.\nThis architecture minimizes financial risk during the MVP (Minimum Viable Product) phase when user traffic is unpredictable. Below is the Unit Economics breakdown for key services:\nService Role Pricing Model Est. Unit Cost AWS Lambda Backend Logic Per request \u0026amp; compute duration ~$0.20 / 1M requests Pinecone Serverless Vector Database (Storage) Per Storage \u0026amp; Read Units ~$0.33 / GB / month Amazon Bedrock LLM Inference (Nova Pro) Per Input/Output Tokens Input: ~$0.0008 / 1k tokens\nOutput: ~$0.0032 / 1k tokens Cohere Embed Embedding Model (Multilingual) Per tokens processed ~$0.10 / 1M tokens DynamoDB Metadata \u0026amp; Cache Write/Read Capacity Units (On-demand) ~$1.25 / 1M Write Units Assessment: With this architecture, the system maintenance cost during idle states is virtually zero.\n2. AI Architecture Table of Contents System Architecture Data Strategy Storage \u0026amp; Retrieval AI Models \u0026amp; RAG CI/CD \u0026amp; Testing Lessons \u0026amp; Roadmap "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.4-models-rag/",
	"title": "AI Models &amp; RAG Pipeline",
	"tags": [],
	"description": "",
	"content": "The system leverages Amazon Bedrock as the centralized Unified API gateway, simplifying version management and credential security.\n1. Model Selection Analysis Embedding: Cohere Embed Multilingual v3 ID: cohere.embed-multilingual-v3 Why Cohere over Titan? Vietnamese/Sino-Vietnamese Nuances: Cohere v3 is trained on a massive multilingual dataset, allowing it to understand specific Sino-Vietnamese esoteric terms (e.g., \u0026ldquo;Thien Di\u0026rdquo;, \u0026ldquo;Phu The\u0026rdquo;) that Western-centric models often misinterpret. Input Type Parameter: Supports input_type=\u0026quot;search_query\u0026quot; (for user questions) and \u0026quot;search_document\u0026quot; (for database ingestion), optimizing the vector space for retrieval tasks. Dimension: 1024 dimensions - The \u0026ldquo;sweet spot\u0026rdquo; balancing semantic accuracy and Pinecone storage costs. Generative LLM: Amazon Nova Pro ID: amazon.nova-pro-v1:0 Reasoning Capability: Unlike simple summarization models, Nova Pro excels at logical chaining. Example: It can synthesize the meanings of 3 separate Tarot cards (Past - Present - Future) into a coherent cause-and-effect narrative. Inference Configuration: temperature: 0.3: Kept low to ensure the AI adheres strictly to the Knowledge Base, minimizing Hallucinations. topP: 0.9: Allows slight stylistic flexibility to make the advice sound natural and empathetic. 2. AWS Lambda Implementation Below is a Python (boto3) code snippet used within the AWS Lambda function to execute the RAG flow: Embed Query -\u0026gt; (Search Pinecone) -\u0026gt; Prompt Nova Pro.\nimport boto3 import json import os # Initialize Bedrock Runtime client bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39; ) def generate_embedding(text): \u0026#34;\u0026#34;\u0026#34; Step 1: Convert user question to Vector \u0026#34;\u0026#34;\u0026#34; response = bedrock.invoke_model( modelId=\u0026#39;cohere.embed-multilingual-v3\u0026#39;, contentType=\u0026#39;application/json\u0026#39;, accept=\u0026#39;application/json\u0026#39;, body=json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34; # Optimized for queries }) ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;embeddings\u0026#39;][0] def query_llm(context_chunk, user_question, history): \u0026#34;\u0026#34;\u0026#34; Step 2: Send Context + Question to Amazon Nova Pro \u0026#34;\u0026#34;\u0026#34; # System Prompt: Define the Persona system_prompt = \u0026#34;\u0026#34;\u0026#34;You are a dedicated Tarot and Astrology expert. Answer the question based ONLY on the information provided in the \u0026lt;context\u0026gt; tags. If the information is insufficient, honestly state that you do not know. Do not fabricate facts.\u0026#34;\u0026#34;\u0026#34; # Construct Message Payload (Nova Pro structure) prompt_payload = { \u0026#34;system\u0026#34;: [{\u0026#34;text\u0026#34;: system_prompt}], \u0026#34;messages\u0026#34;: [ # Chat history can be injected here {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: f\u0026#34;Context: {context_chunk}\\n\\nQuestion: {user_question}\u0026#34;}]} ], \u0026#34;inferenceConfig\u0026#34;: { \u0026#34;temperature\u0026#34;: 0.3, # Reduce hallucinations \u0026#34;maxTokens\u0026#34;: 1000 } } response = bedrock.invoke_model( modelId=\u0026#39;amazon.nova-pro-v1:0\u0026#39;, body=json.dumps(prompt_payload) ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;output\u0026#39;][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.4-deployment/",
	"title": "Setting up CI/CD Pipeline with AWS Amplify",
	"tags": [],
	"description": "",
	"content": "1. CI/CD Process We will set up a fully automated process as follows:\nSource: Developer pushes code to GitHub. Build: Amplify automatically detects changes, installs dependencies, and builds Next.js. Deploy: Pushes built code to global Hosting system. Verify: Checks website health (Health Check). 2. Connect Repository Push code to Git Repository (GitHub/GitLab/CodeCommit). In AWS Amplify Console, select Host web app. Connect to the Repository containing Frontend source code. 3. Build Configuration (Build Specification) Amplify uses amplify.yml to define build steps.\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: .next files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* 4. Environment Variables Management Never hard-code sensitive or environment-specific values (like API URLs) in your code. Use Environment Variables in Amplify.\nGo to App settings \u0026gt; Environment variables. Add variable: Key: NEXT_PUBLIC_API_URL Value: https://xyz.execute-api.us-east-1.amazonaws.com/prod Access in Next.js code via process.env.NEXT_PUBLIC_API_URL. Note: Variables starting with NEXT_PUBLIC_ will be embedded into the Frontend code by Next.js at build time.\n5. Branch Previews (Pull Request Previews) This feature is incredibly useful for team collaboration.\nWhen you create a Pull Request (PR) on GitHub, Amplify automatically creates a temporary Preview environment (with a unique URL). Team Leaders can visit that URL to review new features before Merging into the main branch. After Merging, the Preview environment is automatically deleted. To enable: Go to App settings \u0026gt; Previews \u0026gt; Enable previews.\nMy Experience Build Error on Linux vs Windows My machine uses Windows (case-insensitive), but Amplify runs Linux (case-sensitive). Once I imported Component.tsx but the file was named component.tsx. It ran fine locally, but failed on Amplify with \u0026ldquo;File not found\u0026rdquo;. Lesson: Always name files consistently (PascalCase for Components, camelCase for utils) and double-check when renaming files.\nVerification \u0026amp; Testing Test Case 1: Trigger Build\nEdit a small text line in page.tsx. Commit and Push to GitHub: git push origin main. Go to Amplify Console. Expected Result: See status change to \u0026ldquo;Provisioning\u0026rdquo; -\u0026gt; \u0026ldquo;Building\u0026rdquo; -\u0026gt; \u0026ldquo;Deploying\u0026rdquo;. Test Case 2: Check Logs\nClick on the running build. Open Frontend tab. Expected Result: See green logs (Success) in steps. 2024-05-20T10:00:00.000Z [INFO]: # Executing command: npm run build\r...\r2024-05-20T10:01:00.000Z [INFO]: Compiled successfully "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.4-aws-ssm/",
	"title": "Storing Sensitive Environment Variables",
	"tags": [],
	"description": "",
	"content": "Storing sensitive environment variables (DATABASE_URL, JWT_SECRET) in AWS Systems Manager (SSM) Parameter Store is the security standard for Serverless applications.\n1. Preparation In your serverless.yml, you used the variable ${self:provider.stage} (defaulting to dev if not specified). To be consistent with that configuration, we should define the Key in the format /my-app/\u0026lt;stage\u0026gt;/\u0026lt;key\u0026gt;.\n2. Run CLI Commands to Store Keys You need to run these commands via the AWS CLI after configuring access permissions.\n# NOTE: Replace \u0026lt;YOUR_VALUE\u0026gt; with your actual value. # We will use the default stage \u0026#34;dev\u0026#34; for the workshop environment. # --- 1. Store NeonDB Connection String --- # Use \u0026#34;SecureString\u0026#34; to encrypt the data. aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:pass@ep-xyz.aws.neon.tech/neondb?sslmode=require\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 2. Store JWT Secret --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/jwt_secret\u0026#34; \\ --value \u0026#34;a_very_long_and_secure_jwt_key_314159\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 3. Store Frontend URL (If necessary for CORS) --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/frontend_url\u0026#34; \\ --value \u0026#34;https://your-amplify-app.amplifyapp.com\u0026#34; \\ --type \u0026#34;String\u0026#34; \\ --overwrite 3. Verification After storing, you can run the following command to check if the Parameter has been saved correctly and if the Serverless Framework can access it:\n# Command to check and decrypt the value (requires SSM Read permission) aws ssm get-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --with-decryption Important Note: In serverless.yml, you defined: DATABASE_URL: ${ssm:/my-app/prod/database_url}. To synchronize with the command above, you need to change prod to dev in serverless.yml if you are deploying with the default stage, or run the CLI commands with the prod stage if you wish to keep the serverless.yml file as is.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.5-cicd/",
	"title": "Automation CI/CD",
	"tags": [],
	"description": "",
	"content": "The objective of this section is to establish a process so that every time code is pushed to the main branch, your entire Serverless architecture will be automatically Built, Tested, and Deployed to AWS without manual intervention from a local machine.\n1. Preparing Secrets on the GitHub Repository This is the most crucial step for granting GitHub Actions access to AWS.\nSecret Name Role Notes and Action AWS_ACCESS_KEY_ID AWS Access Key. VERY IMPORTANT: Use an IAM User with minimum necessary permissions (not Admin) to only allow creation/updating of Lambda, API Gateway resources, and reading SSM. AWS_SECRET_ACCESS_KEY Corresponding Secret Key. (Keep as is) SERVERLESS_ACCESS_KEY Serverless Dashboard Key. Only needed if you use Serverless Dashboard management features. If not, this can be omitted. 2. Workflow File .github/workflows/deploy.yml This file defines the steps that GitHub Actions will execute. We will add the Build step and omit environment variables unnecessary for the deploy command.\nname: Deploy Backend CI/CD via Serverless Framework on: push: branches: [ main ] # Triggers when pushing to the main branch workflow_dispatch: # Allows manual triggering from the GitHub UI jobs: deploy: runs-on: ubuntu-latest # Grant permissions for GitHub Actions permissions: id-token: write contents: read steps: - uses: actions/checkout@v4 # 1. Get source code - name: Setup Node 20 uses: actions/setup-node@v3 with: node-version: \u0026#39;20\u0026#39; cache: \u0026#39;npm\u0026#39; # Enable cache to speed up installation - name: Install Dependencies run: npm ci # Use npm ci to ensure consistency (from package-lock.json) # --- PRISMA \u0026amp; CODE BUILD CONFIGURATION --- - name: Generate Prisma Client (download Linux binary) # Download the rhel-openssl-3.0.x binary (necessary for Lambda) run: npx prisma generate - name: Build TypeScript Code (tsc -\u0026gt; dist) # Run the compilation command (assumes a \u0026#34;build\u0026#34;: \u0026#34;tsc\u0026#34; script in package.json) run: npm run build # --- AWS CONFIGURATION --- - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-southeast-1 # --- FINAL DEPLOYMENT --- - name: Deploy Serverless # Use npx to call the serverless CLI run: npx serverless deploy env: # Explicitly specify the deployment stage. SLS_STAGE: dev # [Note]: The DATABASE_URL variable is already handled by placeholder or SSM DATABASE_URL: \u0026#34;placeholder\u0026#34; 3. CI/CD Workflow Summary Code Commit: You develop code and perform a git push to GitHub. Trigger: GitHub Actions automatically triggers the deploy.yml workflow. Build \u0026amp; Package: The Ubuntu Runner installs dependencies, runs npx prisma generate (downloading the Linux binary), and compiles the code. AWS Authentication: aws-actions/configure-aws-credentials logs into AWS using your Secret Keys. Provisioning: The npx serverless deploy command reads serverless.yml, connects to SSM Parameter Store to fetch secret keys, and instructs CloudFormation to build/update the entire Lambda and API Gateway infrastructure. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.5-cicd/",
	"title": "CI/CD &amp; Testing",
	"tags": [],
	"description": "",
	"content": "The system implements a fully automated CI/CD pipeline via GitHub Actions, ensuring that every line of code is rigorously tested (Test) before being deployed to the AWS Lambda environment.\n1. CI Mechanism – Continuous Integration Whenever new code is Pushed or a Pull Request is opened against the main branch, the CI workflow is automatically triggered to verify logic integrity.\nWorkflow: [Code Commit] ➔ [GitHub Actions Trigger] ➔ [Runner Ubuntu Start] ➔ [Setup Python Env] ➔ [Run Pytest] ➔ [Result: ✅ Pass / ❌ Fail]\nMocking Strategy: All Unit Tests utilize unittest.mock to simulate AWS services (S3, DynamoDB, Bedrock). This ensures: Zero Cost: Testing does not incur AWS API fees. High Speed: Tests complete in seconds rather than waiting for network latency. Safety: Production data is never overwritten or deleted during testing. 2. CD Mechanism – Continuous Deployment The CD pipeline is activated to package and deploy code to AWS only when the CI process returns a Success (✅) result.\nWorkflow: [CI Success] ➔ [Install Binary Libs] ➔ [Package Zip 📦] ➔ [AWS CLI Upload 🚀] ➔ [Lambda Live Update]\nNote on Deployment Package: A major challenge with AWS Lambda is the compatibility of C-extension libraries (like numpy for calculations). The system resolves this by installing and building libraries with the --platform manylinux2014_x86_64 flag directly within the CD pipeline to ensure absolute compatibility with the Amazon Linux environment.\n3. Unit Testing Strategy The testing framework covers three logic layers: Input Validation, Happy Path, and Error Handling.\nChatbot Lambda (sorcererxstreme-chatbot) Input Validation: Case 1 (Missing Session): Missing sessionId → Returns 400 Bad Request. Case 2 (Missing Question): Empty question content → Returns 400. Case 3 (Invalid JSON): Malformed JSON body → Returns Parse Error. Happy Path: Case 4: Simulates full RAG flow: Load History → Query Pinecone → Call Bedrock → Save Chat. Asserts that AI returns a 200 OK response. Embedding Lambda (sorcererxstreme-embedding) Logic \u0026amp; Utilities: Case 1 (Flatten Contexts): Verifies JSON flattening. Input {\u0026quot;hobbies\u0026quot;: [\u0026quot;code\u0026quot;, \u0026quot;read\u0026quot;]} must become string \u0026quot;hobbies: code, read\u0026quot;. Case 2 (Bedrock Fail): Simulates Bedrock network error. get_embedding must return None to avoid crashing the Batch process. Integration Flow: Case 3 (Happy Flow): Simulates reading JSONL from S3 → Embedding 2 items → Writing to DynamoDB \u0026amp; Pinecone. Verifies API call counts match item counts. Case 4 (S3 Error): Simulates S3 file not found → System reports 500 (Internal Error). Metaphysical Lambda (sorcererxstreme-metaphysical) This is the most complex function with domain-based routing logic.\nInput Validation: Case 1 (Invalid Domain): Sending domain: \u0026quot;bitcoin\u0026quot; → Returns 400 (Only Tarot, Horoscope, Numerology, Astrology supported). Case 2 (Missing Context): Selecting \u0026ldquo;Horoscope\u0026rdquo; but missing birth_date → Returns input request. Case 3 (Empty Tarot): Selecting \u0026ldquo;Tarot\u0026rdquo; but cards_drawn array is empty → Warns user to select cards. Domain Logic: Case 4 (Tarot Flow): Mocks DynamoDB returning \u0026ldquo;The Sun\u0026rdquo; meaning. Verifies AI receives the correct context for analysis. Case 5 (Horoscope Flow): Verifies integration with lasotuvi library. Mocks \u0026ldquo;Thien Ban\u0026rdquo; object to ensure AI receives \u0026ldquo;Element: Earth\u0026rdquo; (Mệnh Thổ) data. Case 6 (Graceful Degradation): When Bedrock is throttled, the system returns a user-friendly message instead of crashing. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.5-advanced-deployment/",
	"title": "Optimization &amp; Advanced Deployment",
	"tags": [],
	"description": "",
	"content": "After successfully deploying SorcererXtreme to the Internet, our work is not done. To make the product truly \u0026ldquo;Production-Ready\u0026rdquo;, we need to perform advanced tuning.\n1. Custom Domain By default, AWS Amplify provides a long and hard-to-remember URL (e.g., main.d12345.amplifyapp.com). Setting up a custom domain not only makes the app look professional but also improves trust.\nProcess:\nPurchase Domain: You can buy directly on Amazon Route 53 or other providers (Namecheap, GoDaddy). Configure in Amplify: Go to App settings \u0026gt; Domain management. Click Add domain and enter your domain (e.g., sorcererxtreme.vn). DNS Verification: If on Route 53: Amplify auto-configures (Zero-config). If external: Amplify provides a CNAME record for you to add to your provider\u0026rsquo;s DNS manager. SSL/TLS: Amplify automatically issues and manages free SSL certificates, ensuring the green padlock (HTTPS). 2. Environment Variables Management During development, we often use .env.local files to store sensitive Keys. However, these files are not pushed to Git when deploying.\nWhy important?\nSecurity: Hides API keys (like API Gateway Endpoint, Cognito User Pool ID) from public source code. Flexibility: Easily switch configs between Staging and Production environments without changing code. Setup:\nAccess Amplify Console \u0026gt; Select App \u0026gt; Environment variables. Enter corresponding Keys (e.g., NEXT_PUBLIC_API_URL, NEXT_PUBLIC_USER_POOL_ID). Trigger the Build process again for changes to take effect. 3. SEO Optimization for Next.js (Search Engine Optimization) For a B2C user-facing app like Tarot reading, appearing on Google is vital. Next.js (App Router) strongly supports SEO via the Metadata API.\nDynamic Metadata: Instead of static titles, create dynamic titles based on the Tarot card the user draws:\n// app/tarot/[cardId]/page.tsx export async function generateMetadata({ params }) { const card = await getTarotCard(params.cardId); return { title: `Meaning of ${card.name} | SorcererXtreme`, description: `Discover the cosmic message from ${card.name}...`, openGraph: { images: [card.imageUrl], // Image shown when shared on Facebook }, } } 4. Monitoring \u0026amp; Analytics You cannot improve what you do not measure. Use the Monitoring Tab in Amplify to track:\nIncoming Traffic: Number of visitors in real-time. Data Transfer: Bandwidth usage. Error Rate (4XX/5XX): Detect immediately if APIs fail or links break. Access Logs: Download access logs to analyze user behavior (origin country, browser used). Front-end Tip: Always check your Lighthouse score (in Chrome DevTools) after every deploy. A beautiful app that loads slowly will lose users instantly. Optimize images (use WebP/AVIF format) and lazy-load heavy components.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.6-backend-architecture/",
	"title": "Backend Reference Architecture (RAG &amp; Database)",
	"tags": [],
	"description": "",
	"content": "As a modern Frontend Developer, the line between Frontend and Backend is blurring. You don\u0026rsquo;t need to write SQL or manage servers, but you usually must understand the data flow to integrate effectively.\nThe SorcererXtreme system uses an advanced RAG (Retrieval-Augmented Generation) architecture. Let\u0026rsquo;s dissect what happens after you call fetch('/api/chat').\n1. RAG Flow: Why is the AI accurate? If we simply send the question to ChatGPT, it might \u0026ldquo;hallucinate\u0026rdquo; based on old training data. To make the AI act like a \u0026ldquo;Wise Sorcerer\u0026rdquo; knowledgeable in Tarot, we use a 4-step process:\nRetrieval: When User asks \u0026ldquo;What does The Fool card mean?\u0026rdquo;, the question is converted into a vector (numbers). The system searches Pinecone (Vector DB) for Tarot book passages with the most similar meaning. Augmentation: The system combines the original question + found content from Pinecone into a complete Prompt. Prompt: \u0026ldquo;Based on the following knowledge [book excerpt\u0026hellip;], answer the question: What does The Fool card mean?\u0026rdquo; Generation: Send this Prompt to Amazon Bedrock (hosting Claude 3 or Titan models). AI answers based strictly on the provided text, avoiding fabrication. Response: Frontend receives the final answer and renders it. 2. \u0026ldquo;Polyglot Persistence\u0026rdquo; Strategy A large application never uses just one type of Database. We use the right tool for the right job:\nA. NeonDB (Serverless PostgreSQL)\nType: Relational. Data: User Profiles, VIP tiers, Payment transactions. Why: Financial data requires the highest integrity (ACID). SQL is the #1 choice. B. Amazon DynamoDB\nType: NoSQL (Key-Value). Data: Chat History, Activity Logs. Why: Chat generates millions of records. DynamoDB offers limitless scale with single-digit millisecond latency. C. Pinecone\nType: Vector Database. Data: Tarot/Astrology Knowledge (encoded as Vectors). Why: SQL or NoSQL cannot search by \u0026ldquo;meaning\u0026rdquo; (semantic search). Only a Vector DB understands that \u0026ldquo;King of Coins\u0026rdquo; and \u0026ldquo;King of Pentacles\u0026rdquo; are related. 3. Security: The \u0026ldquo;Fortress\u0026rdquo; Model Your Frontend (sorcererxtreme.vn) is public land. But the Backend is a sanctuary.\nNo Exposed Databases: NeonDB or Pinecone never open ports to the Internet. API Gateway as the Guard: All requests must pass through API Gateway. It checks the \u0026ldquo;ID Badge\u0026rdquo; (Cognito Token). If missing or expired -\u0026gt; Block immediately (401 Unauthorized). Lambda as the Courier: Only Lambda functions (residing in a private VPC) hold the Secret Keys to unlock the Database doors. Takeaway: When debugging \u0026ldquo;why did the AI give a wrong answer?\u0026rdquo;, a Frontend Dev can guess: \u0026ldquo;Ah, maybe the Retrieval step in Pinecone found the wrong context\u0026rdquo;, instead of blaming the AI Model. Understanding the system helps you fix bugs faster!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.6-microservice/",
	"title": "Connect microservices",
	"tags": [],
	"description": "",
	"content": "In a Serverless architecture, when one Lambda function (e.g., Chatbot) needs to call another service (MetaphysicalAPI), the best approach is to use an HTTP request through that service\u0026rsquo;s API Gateway.\n1. Principles of API Calling Endpoint: Always call through the public API Gateway Endpoint (or an internal one if both Lambdas are within a VPC and use a Private API Gateway). Library: Use a familiar HTTP library like axios to manage the request and client-side timeout. Security: Use IAM Roles or API Keys to authenticate calls between services if the API Gateway is not entirely public. 2. Timeout Management Managing the timeout is the most critical factor to prevent 504 Gateway Timeout errors.\n[Image of AWS API Gateway 29-second timeout limit diagram]\nAPI Gateway Limit: The API Gateway has a hard timeout limit of 29 seconds. Any request lasting longer than 29 seconds is cut off and returns a 504 error. Lambda Limit: Lambda can run up to 15 minutes, but it is constrained by the API Gateway. Therefore, the Lambda timeout must be set less than 29 seconds (e.g., 25 seconds). Configuration Location Recommended Value Rationale API Gateway Timeout serverless.yml (Provider level) 29 seconds AWS maximum hard limit. Lambda Timeout (Receiver) serverless.yml (Function level) 25 seconds Must be less than 29s so Lambda can return an error before the API Gateway cuts the connection. Client Timeout (axios) TypeScript Code 25,000 ms (25 seconds) Ensures the client cuts the connection before Lambda times out, allowing for cleaner client-side error handling. 3. Lambda Performance Optimization If your AI Service performs heavy tasks (like RAG, astronomical calculations), optimizing processing speed is mandatory.\nIncrease Memory (RAM): Increase the memorySize of the AI Service Lambda to a higher level (e.g., 1536 MB or 3008 MB). On AWS, increasing RAM also increases CPU and Network Bandwidth, significantly speeding up heavy processing and reducing the likelihood of 504 errors. Code Configuration: Configure timeout: 25 in serverless.yml for both the Backend and the AI Service. 4. Code Example Here is the AI calling function, modified to handle exceptions clearly and adhere to the Timeout principle.\nimport axios from \u0026#39;axios\u0026#39;; // Assume AI_SERVICE_URL is retrieved from AWS SSM Parameter Store and injected into ENV const AI_SERVICE_URL = process.env.AI_SERVICE_URL; const CLIENT_TIMEOUT_MS = 25000; // 25 seconds (Below the 29s API Gateway limit) /** * Calls the AI service\u0026#39;s API Gateway to receive a response. * @param prompt The request data sent to the AI. */ export const callAI = async (prompt: string, userId: string) =\u0026gt; { if (!AI_SERVICE_URL) { throw new Error(\u0026#34;AI Service URL is not configured.\u0026#34;); } try { const res = await axios.post(AI_SERVICE_URL, { prompt, userId // Send user ID so the AI Service can log or handle VIPs }, { timeout: CLIENT_TIMEOUT_MS // Set client timeout }); // Handle successful response return res.data; } catch (error) { if (axios.isAxiosError(error) \u0026amp;\u0026amp; error.code === \u0026#39;ECONNABORTED\u0026#39;) { // Client-side Timeout error console.error(\u0026#34;AI Timeout Error: Request took too long.\u0026#34;); throw new Error(\u0026#34;AI Service timed out (504). Please try again.\u0026#34;); } // Other errors (e.g., 4xx, 5xx from AI Service) console.error(\u0026#34;AI Service Error:\u0026#34;, error.message); throw new Error(\u0026#34;AI Service unavailable or returned an error.\u0026#34;); } }; "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.4-ai-development/2.4.6-roadmap/",
	"title": "Lessons &amp; Roadmap",
	"tags": [],
	"description": "",
	"content": "1. Technical Challenges \u0026amp; Critical Lessons Building a metaphysical AI consulting system is not just about assembling AWS services; it is a battle for Data Quality and Semantics.\nThe Issue of Information Noise (RAG Hallucination): Reality: Metaphysical data is often abstract. When a user asks a vague question (e.g., \u0026ldquo;How is my future?\u0026rdquo;), Vector Search easily returns irrelevant results (Noise), causing the LLM to \u0026ldquo;hallucinate\u0026rdquo; and fabricate answers. Lesson: The quality of the Knowledge Base is more important than quantity. Chunking data via .jsonl structure and meticulous Metadata Tagging are the keys to increasing Precision. Language Challenges (Vietnamese Nuance): Reality: International Embedding models sometimes fail to fully grasp Sino-Vietnamese terms in Horoscopes (e.g., \u0026ldquo;Cung Mệnh\u0026rdquo;, \u0026ldquo;Thiên Di\u0026rdquo;). Solution: Use of cohere.embed-multilingual-v3 combined with Hybrid Search (pairing DynamoDB\u0026rsquo;s exact keyword search with Pinecone\u0026rsquo;s semantic search) to compensate for this deficiency. 2. Future Roadmap The next objective is not just for the AI to answer correctly, but to answer \u0026ldquo;correctly specifically for YOU.\u0026rdquo; The system will shift from general consulting to identity-based consulting.\nUser Feedback Loop (RLHF Lite) Implement a Like/Dislike mechanism for each response. This data will be recorded to:\nRefine Prompts (Prompt Tuning). Filter out low-quality RAG data segments from Pinecone. 3. Closing The current system has established a solid foundation: Serverless for cost optimization, Pinecone for memory processing, and Python to connect everything.\nThe shift towards Personalization will be a quantum leap, transforming the system from a \u0026ldquo;Search Engine\u0026rdquo; into a true \u0026ldquo;Spiritual Companion,\u0026rdquo; capable of deep understanding and companionship with the user.\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.2-frontend-development/2.2.7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "After completing the Workshop, cleaning up is mandatory to avoid unexpected AWS charges. Since we use Serverless Services (Amplify, Lambda, Bedrock), cleanup is straightforward but requires attention.\n1. Delete AWS Amplify App This is the most critical step. Deleting the App on Amplify automatically cleans up 80% of related resources (S3 Hosting, CloudFront, CI/CD Pipeline).\nAccess AWS Amplify Console. Select the SorcererXtreme app. Click Actions tab (top right) -\u0026gt; Select Delete app. Type the confirmation phrase (usually delete) and click Confirm. 2. Cleanup Database \u0026amp; External Services Since NeonDB and Pinecone are 3rd party services (not included in Amplify Delete), you must delete them manually:\nNeonDB: Log in to Neon Console. Go to Project Settings -\u0026gt; Delete Project. Pinecone: Log in to Pinecone Console. Delete the Index (e.g., tarot-knowledge-base) to stop vector storage billing. 3. Manual AWS Resource Cleanup (If created separately) If you created extra resources outside of Amplify during the workshop, check:\nAmazon Bedrock: Bedrock is billed On-demand per request, so you don\u0026rsquo;t need to \u0026ldquo;delete\u0026rdquo; Models. However, if you created a custom Knowledge Base, delete it. Amazon Cognito: Check if the User Pool is gone (Amplify usually deletes it). Parameter Store: Go to AWS Systems Manager \u0026gt; Parameter Store \u0026gt; Delete keys like /sorcerer/neon_db_url, /sorcerer/pinecone_api_key. 4. Final Switch (Billing Dashboard) To be 100% sure:\nAccess AWS Billing Dashboard. Check the \u0026ldquo;Bills\u0026rdquo; section. Wait 24h for the system to update and ensure no new charges are appearing from unknown services. Conclusion Congratulations on reaching the end of the journey!\nYou have successfully built SorcererXtreme - an application blending Frontend artistry (Next.js), Cloud power (AWS Amplify), and Artificial Intelligence (Bedrock RAG).\nSee you in advanced Workshops!\n"
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.7-email/",
	"title": "Setup Email",
	"tags": [],
	"description": "",
	"content": "To ensure the highest delivery reliability for project emails and prevent them from being marked as Spam by email service providers (like Gmail, Outlook), domain security configuration is mandatory.\n1. Basic Configuration and Identity Security Step Action Purpose 1. Domain Purchase Use a private domain (.com, .xyz) instead of a shared one. Reputation: Establish a separate email sending reputation, avoiding impact from other bad senders. 2. Verify Domain in AWS SES Access SES -\u0026gt; Verified Identities -\u0026gt; Create Identity (Select Domain). Proof of Ownership: Allows AWS SES to manage email sending on behalf of your domain. 3. Configure DNS (Email Security) Add CNAME (DKIM), TXT (SPF), and TXT (DMARC) records. Crucial: These records verify the sending source (DKIM/SPF) and instruct the receiving server how to handle invalid emails (DMARC), preventing emails from falling into Spam. Details of Mandatory DNS Records: DKIM (CNAME): Add the 3 CNAME records provided by AWS SES. Purpose: Digital Signature. SPF (TXT): Add a TXT record with the content: v=spf1 include:amazonses.com ~all. Purpose: Designates AWS SES as an authorized server to send mail. DMARC (TXT): Add a TXT record (usually as _dmarc) with the content: v=DMARC1; p=none;. Purpose: Establishes reporting and handling policies for fraudulent emails. 2. Setting up the Sending Flow and Exiting Sandbox After the domain has been verified (Verification Status: Verified), you need to set up the operational flow and request actual sending permission.\nStep Interacting Service Action 1. Activate Async Flow EventBridge Scheduler -\u0026gt; Lambda (TriggerReminder) The asynchronous flow starts according to a defined schedule. 2. Send SES Request Lambda (TriggerReminder) -\u0026gt; Amazon SES API The Lambda function (with the granted IAM Role) directly calls the SES API (e.g., SendEmailCommand) to send personalized emails to each user. 3. Request Production Access AWS Support Center Submit a ticket requesting AWS to upgrade your account from Sandbox mode so you can send emails to any unverified address. "
},
{
	"uri": "http://localhost:1313/repo-name/2-workshop/2.3-backend-development/2.3.8-summary/",
	"title": "Development workflow",
	"tags": [],
	"description": "",
	"content": "The workflow is designed to leverage the speed of local development and the safety and automation of the Serverless architecture.\nStep Activity Environment Role and Notes 1. Code Development Code logic, API modifications (TypeScript/Express). Local Machine Use VS Code and Node.js libraries. 2. Local Testing Test synchronous APIs. sls offline start Simulates the Lambda/API Gateway environment. Connects directly to NeonDB via the local .env file. 3. Database Schema Update If Schema is modified (schema.prisma). npx prisma migrate dev Creates Migration and applies changes. 4. Commit \u0026amp; Push Code Commit code and push to the repository. git push origin main Triggers the automated CI/CD flow. 5. Automated CI/CD Deployment to the Cloud. GitHub Actions Automated: Build -\u0026gt; Prisma Generate -\u0026gt; AWS/SSM Login (fetch Keys) -\u0026gt; Deploy to AWS Lambda. 6. Error Monitoring Check Real-time Logs. sls logs -f api -t Use the Serverless Framework command to immediately view CloudWatch Logs and debug errors in the Production/Staging environment. Important Notes on Prisma: Use migrate dev: Instead of db push (which is only for non-production/test), you should use npx prisma migrate dev to create a history of changes (migration files) and apply them to NeonDB. Generate on CI/CD: Running npx prisma generate in GitHub Actions is mandatory to download the necessary binaries (rhel-openssl-3.0.x) for the AWS Lambda Linux environment. "
},
{
	"uri": "http://localhost:1313/repo-name/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/repo-name/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]